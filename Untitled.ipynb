{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef57664a-141d-4a36-8567-4da43e8495ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/500], Loss: 0.7803\n",
      "Epoch [10/500], Loss: 0.0796\n",
      "Epoch [20/500], Loss: 0.0747\n",
      "Epoch [30/500], Loss: 0.0623\n",
      "Epoch [40/500], Loss: 0.0469\n",
      "Epoch [50/500], Loss: 0.0372\n",
      "Epoch [60/500], Loss: 0.0316\n",
      "Epoch [70/500], Loss: 0.0287\n",
      "Epoch [80/500], Loss: 0.0273\n",
      "Epoch [90/500], Loss: 0.0259\n",
      "Epoch [100/500], Loss: 0.0247\n",
      "Early stopping triggered.\n",
      "The regressor MLP() has been trained.\n",
      "The regressor ML_XGBoost() has been trained.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time  \n",
    "import random\n",
    "import xgboost\n",
    "import sklearn\n",
    "from itertools import product\n",
    "from itertools import chain\n",
    "\n",
    "#from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "def run(data, models, model_list, desired, starting_point, mode, modeling, strategy, tolerance, beam_width,\n",
    "        num_cadidates, escape, top_k, index, up, alternative):\n",
    "    \n",
    "    neural_list = ['MLP','Conv']\n",
    "\n",
    "    regression_epochs = 500\n",
    "    regression_eta = 0.01\n",
    "    regression_show_epochs = True \n",
    "    regression_dropout = 0.3\n",
    "    regression_batch = 256\n",
    "    regression_hidden_size = 32\n",
    "\n",
    "    configuration_patience = 10\n",
    "    configuration_patience_volume = 0.01\n",
    "    configuration_steps = 100\n",
    "    configuration_eta = 10\n",
    "    configuration_eta_decay = 0.001\n",
    "    configuration_show_steps = True\n",
    "    configuration_show_focus = False\n",
    "    configuration_tolerance = tolerance\n",
    "    configuration_retrial_threshold = 10\n",
    "    start_from_standard = False\n",
    "    start_from_random = False\n",
    "    constrain_reselection = True\n",
    "\n",
    "    if_visualize = True\n",
    "\n",
    "    seed = 2024\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "    output_size = 1\n",
    "    input_size = data.values.shape[1] - output_size\n",
    "    dtype = torch.float32\n",
    "    \n",
    "    constraints = {'ATT1' : [5, min(data.iloc[:,0+1]), max(data.iloc[:,0+1]), int, 0, 150],\n",
    "                  'ATT2'  : [5, min(data.iloc[:,1+1]), max(data.iloc[:,1+1]), int, 0, 25],\n",
    "                  'ATT3'  : [10, min(data.iloc[:,2+1]), max(data.iloc[:,2+1]), int, -1, 40],\n",
    "                  'ATT4'  : [0.5, min(data.iloc[:,3+1]), max(data.iloc[:,3+1]), float, 1, 1],\n",
    "                  'ATT5'  : [5, min(data.iloc[:,4+1]), max(data.iloc[:,4+1]), int, 0, 120],\n",
    "                  'ATT6'  : [5, min(data.iloc[:,5+1]), max(data.iloc[:,5+1]), int, 0, 250],\n",
    "                  'ATT7'  : [5, min(data.iloc[:,6+1]), max(data.iloc[:,6+1]), int, 0, 10],\n",
    "                  'ATT8'  : [5, min(data.iloc[:,7+1]), max(data.iloc[:,7+1]), int, 0, 25],\n",
    "                  'ATT9'  : [5, min(data.iloc[:,8+1]), max(data.iloc[:,8+1]), int, 0, 25],\n",
    "                  'ATT10' : [60, min(data.iloc[:,9+1]), max(data.iloc[:,9+1]), int, -1, 900],\n",
    "                  'ATT11' : [0.05, min(data.iloc[:,10+1]), max(data.iloc[:,10+1]), float, 2, 0.25],\n",
    "                  'ATT12' : [1, min(data.iloc[:,11+1]), max(data.iloc[:,11+1]), int, 0, 2],\n",
    "                  'ATT13' : [5, min(data.iloc[:,12+1]), max(data.iloc[:,12+1]), int, 0, 100],\n",
    "                  'ATT14' : [60, min(data.iloc[:,13+1]), max(data.iloc[:,13+1]), int, -1, 1800],\n",
    "                  'ATT15' : [1000, min(data.iloc[:,14+1]), max(data.iloc[:,14+1]), int, 0, 2000]}\n",
    "\n",
    "    starting_point = pd.DataFrame(np.array(starting_point).reshape(1,-1), columns = [f'ATT{i+1}' for i in range(input_size)])\n",
    "    erase = []\n",
    "    for i in range(input_size):\n",
    "        if min(data.iloc[:,i+1]) == max(data.iloc[:,i+1]):\n",
    "            erase.append(f\"att{i+1}\")\n",
    "            del constraints[f'ATT{i+1}']\n",
    "            del starting_point[f'ATT{i+1}']\n",
    "                               \n",
    "    \n",
    "    starting_point = starting_point.values.reshape(-1).tolist()\n",
    "    data = data.drop(columns=erase)\n",
    "    input_size = data.values.shape[1] - output_size    \n",
    "    \n",
    "    class MinMaxScaling :\n",
    "        def __init__(self, data): #np.DataFrame\n",
    "            self.max, self.min, self.range = [],[], []\n",
    "            self.data = pd.DataFrame([])\n",
    "            data = data.values.reshape(-1,1) if len(data.values.shape) == 1 else data.values\n",
    "\n",
    "            epsilon = 2\n",
    "            for i in range(data.shape[1]) :\n",
    "                max_, min_ = max(data[:,i]), min(data[:,i])\n",
    "                if max_ == min_ : max_ *= epsilon\n",
    "                self.max.append(max_)\n",
    "                self.min.append(min_)\n",
    "                self.range.append(max_-min_)\n",
    "                self.data = pd.concat([self.data, pd.DataFrame((data[:,i])/(max_-min_))],axis = 1)\n",
    "            self.data = torch.tensor(self.data.values, dtype = dtype)\n",
    "\n",
    "        def denormalize(self, data):\n",
    "            data = data.detach().numpy() if isinstance(data, torch.Tensor) else data\n",
    "            new_data = []\n",
    "            for i, element in enumerate(data):\n",
    "                element = (element * (self.max[i] - self.min[i])) \n",
    "                element = round(element, np.array(list(constraints.values()))[:,4][i])\n",
    "                new_data.append(element)\n",
    "            return new_data\n",
    "        \n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size = input_size, output_size = output_size, hidden_size = regression_hidden_size, n_layers = 1):\n",
    "            super(MLP, self).__init__()\n",
    "            self.first_layer = nn.Linear(input_size, hidden_size)\n",
    "            self.layers = []\n",
    "            self.layers_dropout =[]\n",
    "\n",
    "            if n_layers > 1 : \n",
    "                self.layers += [nn.Linear(hidden_size, hidden_size) for _ in range(n_layers - 1)]\n",
    "           #     self.layers_dropout += [nn.Dropout(regression_dropout) for _ in range(n_layers - 1)]\n",
    "\n",
    "            self.last_layer = nn.Linear(hidden_size, output_size)\n",
    "          #  self.dropout_input = nn.Dropout(regression_dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.first_layer(x))\n",
    "        #    x = self.dropout_input(x)\n",
    "            for i in range(len(self.layers)):\n",
    "                layer = self.layers[i]\n",
    "           #     drop = self.layers_dropout[i]\n",
    "                x = torch.relu(layer(x))\n",
    "           #     x = drop(x)\n",
    "\n",
    "            x = self.last_layer(x)\n",
    "            return x\n",
    "        \n",
    "    def ML_XGBoost():\n",
    "        return xgboost.XGBRegressor()\n",
    "\n",
    "    def ML_LinearRegressor():\n",
    "        return LinearRegression()\n",
    "\n",
    "    def ML_Ridge():\n",
    "        return Ridge(alpha=1.0)\n",
    "\n",
    "    def ML_Lasso():\n",
    "        return Lasso(alpha = 0.1)\n",
    "\n",
    "    def ML_DecisionTreeRegressor():\n",
    "        return DecisionTreeRegressor(max_depth = 5)\n",
    "\n",
    "    def ML_RandomForestRegressor():\n",
    "        return RandomForestRegressor()\n",
    "\n",
    "    def ML_GradientBoostingRegressor():\n",
    "        return GradientBoostingRegressor()\n",
    "\n",
    "    def ML_SVR():\n",
    "        return SVR(kernel='rbf')\n",
    "\n",
    "    def ML_KNeighborsRegressor():\n",
    "        return KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "    def ML_HuberRegressor():\n",
    "        return HuberRegressor()\n",
    "\n",
    "    def ML_GaussianProcessRegressor():\n",
    "        return GaussianProcessRegressor()\n",
    "     \n",
    "    def modeller(model) :\n",
    "        if model == 'MLP()':\n",
    "            x = MLP()\n",
    "        elif model == 'ML_XGBoost()':\n",
    "            x = ML_XGBoost()\n",
    "        elif model == 'MLP(n_layers = 2)':\n",
    "            x = MLP(n_layers=2)\n",
    "        elif model == 'MLP(n_layers = 3)':\n",
    "            x = MLP(n_layers=3)\n",
    "        elif model == 'ML_LinearRegressor()':\n",
    "            x = ML_LinearRegressor()\n",
    "        elif model == 'ML_Ridge()':\n",
    "            x = ML_Ridge()\n",
    "        elif model == 'ML_Lasso()':\n",
    "            x = ML_Lasso()\n",
    "        elif model == 'ML_DecisionTreeRegressor()':\n",
    "            x = ML_DecisionTreeRegressor()\n",
    "        elif model == 'ML_RandomForestRegressor()':\n",
    "            x = ML_RandomForestRegressor()\n",
    "        elif model == 'ML_GradientBoostingRegressor()':\n",
    "            x = ML_GradientBoostingRegressor()\n",
    "        elif model == 'ML_SVR()':\n",
    "            x = ML_SVR()\n",
    "        elif model == 'ML_KNeighborsRegressor()':\n",
    "            x = ML_KNeighborsRegressor()\n",
    "        elif model == 'ML_HuberRegressor()':\n",
    "            x = ML_HuberRegressor()\n",
    "        elif model == 'ML_GaussianProcessRegressor()':\n",
    "            x = ML_GaussianProcessRegressor()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {model}\")\n",
    "            \n",
    "        return x\n",
    "    class EarlyStopping:\n",
    "        def __init__(self, patience=10, delta=0.1):\n",
    "            self.patience = patience\n",
    "            self.delta = delta\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.train_loss_min = float('inf')\n",
    "\n",
    "        def __call__(self, train_loss):\n",
    "            score = -train_loss\n",
    "\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.train_loss_min = train_loss\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "                self.train_loss_min = train_loss\n",
    "                \n",
    "    def train(feature, target):\n",
    "        trained_models = []\n",
    "        all_training_losses = []\n",
    "        for i, model in enumerate(models):\n",
    "            if any([m in model_list[i] for m in neural_list]): \n",
    "                training_losses = _train_nn(model, feature, target)\n",
    "                all_training_losses.append(training_losses)\n",
    "            else: \n",
    "                _train_ml(model, feature, target)\n",
    "                all_training_losses.append(None)\n",
    "            trained_models.append(model)\n",
    "            print(f\"The regressor {model_list[i]} has been trained.\")\n",
    "        return trained_models, all_training_losses\n",
    "\n",
    "    def _train_nn(model, feature, target, epochs = regression_epochs):\n",
    "        optimizer = optim.Adam(model.parameters(), lr=regression_eta)\n",
    "        criterion = nn.MSELoss() \n",
    "        early_stopping = EarlyStopping()\n",
    "\n",
    "        training_losses = []\n",
    "        Batch = []\n",
    "        for indexer in range((len(feature) // regression_batch) + 1):\n",
    "            Batch.append([feature[indexer * regression_batch : (indexer+1) * regression_batch,:],\n",
    "                        target[indexer * regression_batch : (indexer+1) * regression_batch,:]])\n",
    "        for epoch in range(regression_epochs):\n",
    "            train_loss = 0\n",
    "            for fea, tar in Batch:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(feature)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(Batch)\n",
    "            if regression_show_epochs and epoch % 10 == 0: print(f\"Epoch [{epoch}/{epochs}], Loss: {train_loss:.4f}\")    \n",
    "            early_stopping(train_loss)\n",
    "            training_losses.append(train_loss)\n",
    "            if epoch > 100 and early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered.\")\n",
    "           #     model.eval()\n",
    "                break\n",
    "      #  model.eval()\n",
    "        return training_losses\n",
    "    def _train_ml(model, feature, target):\n",
    "        model.fit(feature, target)       \n",
    "        \n",
    "    class UNIVERSE :\n",
    "        def __init__(self, constraints = constraints):\n",
    "            self.constraints = constraints\n",
    "            self.unit_by_feature = [np.array(list(constraints.values()))[:,0][i] / (feature.max[i] - feature.min[i])\n",
    "                                    for i in range(input_size)]\n",
    "            self.upper_bounds = [np.array(list(constraints.values()))[:,2][i] / (feature.max[i] - feature.min[i])  \n",
    "                                 for i in range(input_size)]\n",
    "            self.lower_bounds = [np.array(list(constraints.values()))[:,1][i] / (feature.max[i] - feature.min[i])  \n",
    "                                 for i in range(input_size)]    \n",
    "\n",
    "            self.raw_unit = np.array(list(constraints.values()))[:,0].tolist()\n",
    "            self.raw_lower = np.array(list(constraints.values()))[:,1].tolist()\n",
    "            self.raw_upper = np.array(list(constraints.values()))[:,2].tolist()\n",
    "\n",
    "        def predict(self, models, x, y_prime, modeling, fake_gradient = True):\n",
    "            predictions,gradients = [], []\n",
    "            copy = x.clone()\n",
    "            for i, model in enumerate(models):\n",
    "                x = x.clone().detach().requires_grad_(True)\n",
    "                if any([m in model_list[i] for m in neural_list]): \n",
    "                    prediction = model(x)\n",
    "                    loss = abs(prediction - y_prime)\n",
    "                    loss.backward()\n",
    "                    gradient = x.grad.detach().numpy()\n",
    "                    prediction = prediction.detach().numpy()\n",
    "\n",
    "                else: # ML\n",
    "                    x = x.detach().numpy().reshape(1,-1)\n",
    "                    prediction = model.predict(x)\n",
    "                    if fake_gradient :\n",
    "                        gradient = []\n",
    "                        for j in range(x.shape[1]):\n",
    "                            new_x = x.copy()\n",
    "                            new_x[0,j] += self.unit_by_feature[j]\n",
    "                            new_prediction = model.predict(new_x)\n",
    "                            slope = (new_prediction - prediction) / self.unit_by_feature[j]\n",
    "                            gradient.append(slope)\n",
    "                        gradient = np.array(gradient).reshape(-1)   \n",
    "                    else : gradient = np.repeat(0, x.shape[1])\n",
    "                x = copy.clone()\n",
    "                predictions.append(prediction)\n",
    "                gradients.append(gradient)\n",
    "\n",
    "            if modeling == 'single': return predictions[0], gradients[0]\n",
    "            elif modeling == 'averaging': return sum(predictions)/len(predictions), sum(gradients)/len(gradients)   \n",
    "            elif modeling == 'ensemble': return \"TODO\"\n",
    "            else: raise Exception(f\"[modeling error] there is no {modeling}.\")\n",
    "\n",
    "\n",
    "        def bounding(self, configuration):\n",
    "            new = []\n",
    "            for k, element in enumerate(configuration):\n",
    "                element = element - (element % self.unit_by_feature[k])\n",
    "                if element >= self.upper_bounds[k] : element = self.upper_bounds[k]\n",
    "                elif element <= self.lower_bounds[k] : element = self.lower_bounds[k]\n",
    "                else: pass\n",
    "                new.append(element)        \n",
    "            configuration = torch.tensor(new, dtype = dtype)        \n",
    "            return configuration\n",
    "\n",
    "\n",
    "        def truncate(self, configuration):\n",
    "            new_configuration = []\n",
    "            for i, value in enumerate(configuration) :\n",
    "                value = value - (value % self.raw_unit[i])\n",
    "                value = value if value >= self.raw_lower[i] else self.raw_lower[i]\n",
    "                value = value if value <= self.raw_upper[i] else self.raw_upper[i]\n",
    "                new_configuration.append(value)\n",
    "           # configuration = torch.tensor(new_configuration, dtype = dtype)     \n",
    "            return configuration\n",
    "        \n",
    "    class LocalMode(UNIVERSE):\n",
    "        def __init__(self, desired, models, modeling, strategy):\n",
    "            super().__init__()\n",
    "            self.desired = desired\n",
    "            self.y_prime = self.desired / (target.max[0] - target.min[0])\n",
    "            self.models = []\n",
    "            self.modeling = modeling\n",
    "            self.strategy = strategy\n",
    "            for model in models : \n",
    "                try: model.eval()\n",
    "                except: pass\n",
    "                self.models.append(model)\n",
    "\n",
    "        def exhaustive(self, starting_point = np.array(list(constraints.values()))[:,5].tolist(), top_k = 5, alternative = 'keep_move'):\n",
    "            self.starting_point = super().truncate(starting_point)\n",
    "            self.starting_point = np.array([self.starting_point[i] / (feature.max[i] - feature.min[i]) \n",
    "                                            for i in range(input_size)])\n",
    "            self.top_k = top_k\n",
    "            self.recorder = []\n",
    "\n",
    "            self.search_space, self.counter = [],[]\n",
    "            if alternative == 'keep_up_down' :\n",
    "                variables = [[0, 1, 2]] * input_size\n",
    "            else :\n",
    "                variables = [[0, 1]] * input_size\n",
    "            self.all_combinations = list(product(*variables))\n",
    "            self.adj = []\n",
    "\n",
    "            if alternative == 'keep_move' or alternative == 'keep_up_down':\n",
    "                prediction_km, gradient_km = super().predict(self.models,torch.tensor(self.starting_point, dtype = dtype), \n",
    "                                               self.y_prime, self.modeling, fake_gradient = True)\n",
    "                prediction_km = prediction_km[0] if isinstance(prediction_km,list) else prediction_km\n",
    "            for combination in self.all_combinations :\n",
    "                count = combination.count(1) if alternative == 'up_down' else combination.count(0)\n",
    "                adjustment = np.repeat(0,len(self.unit_by_feature)).tolist()\n",
    "                for i, boolean in enumerate(list(combination)):\n",
    "                    if alternative == 'up_down':\n",
    "                        if boolean == 1 :   adjustment[i] = adjustment[i] + self.unit_by_feature[i]\n",
    "                        elif boolean == 0 : adjustment[i] = adjustment[i] - self.unit_by_feature[i]\n",
    "                        else: raise Exception(\"ERROR\")\n",
    "\n",
    "                    elif alternative == 'keep_move':\n",
    "                        if prediction_km > self.y_prime :                        \n",
    "                            if boolean == 1 :   \n",
    "                                if gradient_km[i] >= 0 :\n",
    "                                    adjustment[i] = adjustment[i] - self.unit_by_feature[i]\n",
    "                                else:\n",
    "                                    adjustment[i] = adjustment[i] + self.unit_by_feature[i]\n",
    "                            elif boolean == 0 : pass\n",
    "                            else: raise Exception(\"ERROR\")            \n",
    "\n",
    "                        else: \n",
    "                            if boolean == 1 :   \n",
    "                                if gradient_km[i] >= 0 :\n",
    "                                    adjustment[i] = adjustment[i] + self.unit_by_feature[i]\n",
    "                                else:\n",
    "                                    adjustment[i] = adjustment[i] - self.unit_by_feature[i]\n",
    "                            elif boolean == 0 : pass\n",
    "                            else: raise Exception(\"ERROR\")    \n",
    "\n",
    "                    elif alternative == 'keep_up_down' :\n",
    "                        important_features = np.argsort(abs(gradient_km))[::-1][:self.top_k]\n",
    "                        if i in important_features :\n",
    "                            if boolean == 2 :   adjustment[i] = adjustment[i] + self.unit_by_feature[i]\n",
    "                            elif boolean == 1 : adjustment[i] = adjustment[i] - self.unit_by_feature[i]\n",
    "                            elif boolean == 0 : pass\n",
    "                            else: raise Exception(\"ERROR\")   \n",
    "\n",
    "                self.adj.append(adjustment)\n",
    "                candidate = self.starting_point + adjustment         \n",
    "                candidate = super().bounding(candidate)\n",
    "                if str(candidate) not in self.recorder : \n",
    "                    self.search_space.append(candidate)\n",
    "                    self.counter.append(count)\n",
    "                    self.recorder.append(str(candidate))\n",
    "        #    print(len(self.search_space))\n",
    "            self.predictions = []\n",
    "            self.configurations = []\n",
    "            for candidate in self.search_space :\n",
    "                prediction, _ = super().predict(self.models,candidate, self.y_prime, self.modeling, fake_gradient = False)\n",
    "                prediction = target.denormalize(prediction)[0]\n",
    "                configuration = feature.denormalize(candidate)\n",
    "                self.predictions.append(prediction)\n",
    "                self.configurations.append(configuration)\n",
    "\n",
    "            self.table = pd.DataFrame({'configurations':self.configurations,'find_dup' : self.configurations,\n",
    "                                      'predictions':self.predictions,'difference' : np.array(abs(np.array(self.predictions)-self.desired)).tolist(),\n",
    "                                      'counter':self.counter})\n",
    "            self.table['find_dup'] = self.table['find_dup'].apply(lambda x: str(x))\n",
    "            self.table = self.table[~self.table.duplicated(subset='find_dup', keep='first')]\n",
    "            self.table = self.table.drop(columns=['find_dup'])\n",
    "\n",
    "            self.table = self.table.sort_values(by='counter', ascending=False).sort_values(by='difference', ascending=True)\n",
    "            self.configurations = self.table['configurations']\n",
    "            self.predictions = self.table['predictions']\n",
    "            self.difference = self.table['difference']\n",
    "            self.counter = self.table['counter']\n",
    "            \n",
    "            configurations = self.configurations[:].values.tolist()\n",
    "            predictions = self.predictions[:].values.tolist()\n",
    "            best_config = configurations[0]\n",
    "            best_pred = predictions[0]\n",
    "\n",
    "            try: return configurations, predictions, best_config, best_pred\n",
    "            except : return self.configurations[:].values.tolist(), self.predictions[:].values.tolist(), best_config, best_pred\n",
    "\n",
    "        def manual(self, starting_point = np.array(list(constraints.values()))[:,5].tolist(), index=0, up=True):\n",
    "            self.starting_point = super().truncate(starting_point)\n",
    "            self.starting_point = np.array([self.starting_point[i] / (feature.max[i] - feature.min[i]) \n",
    "                                            for i in range(input_size)])\n",
    "\n",
    "            adjustment = np.repeat(0,len(self.unit_by_feature)).tolist()\n",
    "            if up : adjustment[index] += self.unit_by_feature[index]\n",
    "            else : adjustment[index] -= self.unit_by_feature[index]\n",
    "            position = self.starting_point + adjustment         \n",
    "            position = super().bounding(position)        \n",
    "            prediction, _ = super().predict(self.models,position, self.y_prime, self.modeling)\n",
    "            prediction = target.denormalize(prediction)\n",
    "            configuration = feature.denormalize(position)\n",
    "            return [configuration], prediction, configuration, prediction\n",
    "        \n",
    "    class GlobalMode(UNIVERSE):\n",
    "        def __init__(self, desired, models, modeling, strategy, tolerance = configuration_tolerance, steps = configuration_steps):\n",
    "            super().__init__()\n",
    "            self.desired = desired\n",
    "            self.y_prime = self.desired / (target.max[0] - target.min[0])\n",
    "            self.models = []\n",
    "            self.modeling = modeling\n",
    "            self.strategy = strategy\n",
    "            self.tolerance = tolerance / (target.max[0] - target.min[0])\n",
    "            self.steps = steps\n",
    "            for model in models : \n",
    "                try: model.eval()\n",
    "                except: pass\n",
    "                self.models.append(model)\n",
    "\n",
    "        def predict_global(self, models, x, y_prime):\n",
    "\n",
    "            predictions,gradients = [], []\n",
    "            copy = x.clone()\n",
    "            for i, model in enumerate(models):\n",
    "                x = x.clone().detach().requires_grad_(True)\n",
    "                if any([m in model_list[i] for m in neural_list]): \n",
    "                    prediction = model(x)\n",
    "                    loss = prediction - y_prime\n",
    "                    loss.backward()\n",
    "                    gradient = x.grad.detach().numpy()\n",
    "                    prediction = prediction.detach().numpy()\n",
    "\n",
    "                else: # ML\n",
    "                    x = x.detach().numpy().reshape(1,-1)\n",
    "                    prediction = model.predict(x)\n",
    "                    gradient = []\n",
    "                    for j in range(x.shape[1]):\n",
    "                        new_x = x.copy()\n",
    "                        new_x[0,j] += self.unit_by_feature[j]\n",
    "                        new_prediction = model.predict(new_x)\n",
    "                        slope = (new_prediction - prediction) / self.unit_by_feature[j]\n",
    "                        gradient.append(slope)\n",
    "                    gradient = np.array(gradient).reshape(-1)   \n",
    "                x = copy.clone()\n",
    "                predictions.append(prediction)\n",
    "                gradients.append(gradient)\n",
    "            return predictions, gradients\n",
    "\n",
    "        def beam(self, beam_width = 5, starting_point = np.array(list(constraints.values()))[:,5].tolist()) :\n",
    "            self.beam_width = beam_width\n",
    "            y_prime = self.desired / (target.max[0] - target.min[0])\n",
    "            tolerance = self.tolerance\n",
    "            final = None\n",
    "\n",
    "            x_i = [starting_point[i] / (feature.max[i] - feature.min[i]) for i in range(input_size)]  \n",
    "            x_prime = torch.tensor([x_i[i] - (x_i[i] % self.unit_by_feature[i]) for i in range(len(self.unit_by_feature))], \n",
    "                                   dtype = dtype)\n",
    "\n",
    "            self.beam_positions, self.beam_targets = [], []\n",
    "            self.beam_positions_denorm, self.beam_targets_denorm = [], []\n",
    "\n",
    "            self.beam_history = []\n",
    "            self.previous_gradient = [[], [], [], [], []]\n",
    "\n",
    "            success = [False]\n",
    "            which = []\n",
    "            close = False\n",
    "            close_margin = 10\n",
    "            for step in range(self.steps):\n",
    "                if len(self.beam_positions) == 0 :\n",
    "                    current_positions = [x_prime.clone().detach()]\n",
    "                    for j in range(self.beam_width - 1):\n",
    "                        random_offsets = torch.tensor(np.random.uniform(-0.5, 0.5, input_size), dtype=x_prime.dtype)\n",
    "                        current_positions += [x_prime.clone().detach() + random_offsets]\n",
    "                else :\n",
    "                    current_positions = self.beam_positions[-1]\n",
    "\n",
    "                configurations = []\n",
    "                candidates = []\n",
    "                candidates_score = []\n",
    "                beam_predictions = []\n",
    "                beams = []\n",
    "                for p, current_pos in enumerate(current_positions):\n",
    "                    configuration = feature.denormalize(current_pos.clone().detach())\n",
    "                    configurations.append(configuration)\n",
    "\n",
    "                    predictions, gradients = self.predict_global(self.models, x = current_pos, y_prime = y_prime)                \n",
    "                    prediction_avg = sum(predictions)/len(predictions) ###\n",
    "                    gradient_avg = sum(gradients)/len(gradients)  \n",
    "\n",
    "                    prediction_original = prediction_avg * (target.max[0] - target.min[0])\n",
    "                    prediction_original = prediction_original[0]\n",
    "                    beam_predictions.append(prediction_original)     \n",
    "\n",
    "                    if abs(prediction_original - self.desired) < close_margin : close = True\n",
    "                    else : close = False\n",
    "                #    print(close)\n",
    "                    if abs(prediction_avg - y_prime) < tolerance:\n",
    "                        best_config = configuration\n",
    "                        best_pred = prediction_original\n",
    "                        success.append(True)\n",
    "\n",
    "               #     if close :\n",
    "               #         order = np.argsort(abs(gradient_avg))\n",
    "               #     else :\n",
    "               #         order = np.argsort(abs(gradient_avg))[::-1]\n",
    "                    order = np.argsort(abs(gradient_avg))[::-1]\n",
    "                    beam = order[:self.beam_width]\n",
    "\n",
    "                    for b in beam:\n",
    "\n",
    "                        adjustment = list(np.repeat(0,len(self.unit_by_feature)))\n",
    "                        if gradient_avg[b] >= 0 :\n",
    "                            adjustment[b] += self.unit_by_feature[b]\n",
    "                        else:\n",
    "                            adjustment[b] -= self.unit_by_feature[b]    \n",
    "\n",
    "\n",
    "                        adjustment = np.array(adjustment)\n",
    "                        if prediction_avg > y_prime : \n",
    "                            position = current_pos.clone().detach() - adjustment \n",
    "                        else :\n",
    "                            position = current_pos.clone().detach() + adjustment \n",
    "\n",
    "                        position = super().bounding(position)\n",
    "                        candidates.append(position)\n",
    "                        candidates_score.append(abs(gradient_avg[b]))\n",
    "\n",
    "                if step % 10 == 0 : print(f\"Step {step} Target : {self.desired}, Prediction : {beam_predictions}\")\n",
    "                select = np.argsort(candidates_score)[::-1][:self.beam_width]\n",
    "                new_positions = [torch.tensor(candidates[s], dtype = dtype) for s in select]\n",
    "\n",
    "                if len(beam_predictions) == 1 : beam_predictions = list(np.repeat(beam_predictions[0],self.beam_width))\n",
    "                self.beam_positions.append(new_positions)\n",
    "                self.beam_targets.append(beam_predictions)\n",
    "                self.beam_history.append(beam.tolist())\n",
    "                self.beam_positions_denorm.append(configurations) \n",
    "                self.beam_targets_denorm.append(beam_predictions)\n",
    "\n",
    "                if any(success): break      \n",
    "\n",
    "            flattened_positions = list(chain.from_iterable(G.beam_positions_denorm))\n",
    "            flattened_predictions = list(chain.from_iterable(G.beam_targets_denorm))\n",
    "            best = np.argsort(abs(np.array(flattened_predictions)-self.desired))[0]\n",
    "\n",
    "            self.best_position = flattened_positions[best]\n",
    "            self.best_prediction = flattened_predictions[best]\n",
    "\n",
    "            return self.beam_positions_denorm, self.beam_targets_denorm, self.best_position, self.best_prediction\n",
    "\n",
    "\n",
    "        def stochastic(self, num_candidates = 5, starting_point = np.array(list(constraints.values()))[:,5].tolist()) :\n",
    "            self.num_candidates = num_candidates\n",
    "            y_prime = self.desired / (target.max[0] - target.min[0])\n",
    "            tolerance = self.tolerance\n",
    "            final = None\n",
    "\n",
    "            x_i = [starting_point[i] / (feature.max[i] - feature.min[i]) for i in range(input_size)]  \n",
    "            x_prime = torch.tensor([x_i[i] - (x_i[i] % self.unit_by_feature[i]) for i in range(len(self.unit_by_feature))], \n",
    "                                   dtype = dtype)\n",
    "\n",
    "            self.stochastic_chosen = []\n",
    "            self.stochastic_predictions = []\n",
    "            self.stochastic_configurations = []\n",
    "\n",
    "            for step in range(self.steps):\n",
    "                configuration = feature.denormalize(x_prime)\n",
    "                predictions, gradients = self.predict_global(self.models, x = x_prime, y_prime = y_prime)\n",
    "                prediction_avg = sum(predictions)/len(predictions) \n",
    "                gradient_avg = sum(gradients)/len(gradients)\n",
    "\n",
    "                candidates = np.argsort(abs(gradient_avg))[::-1][:self.num_candidates] #[::-1]\n",
    "                chosen = random.choice(candidates)\n",
    "\n",
    "                adjustment = list(np.repeat(0,len(self.unit_by_feature)))\n",
    "\n",
    "                if gradient_avg[chosen] >= 0: adjustment[chosen] += self.unit_by_feature[chosen]\n",
    "                else: adjustment[chosen] -= self.unit_by_feature[chosen]\n",
    "                adjustment = np.array(adjustment)\n",
    "\n",
    "                if prediction_avg > y_prime: x_prime -= adjustment \n",
    "                elif prediction_avg < y_prime: x_prime += adjustment\n",
    "                else: pass\n",
    "                x_prime = super().bounding(x_prime)\n",
    "\n",
    "                prediction_original = target.denormalize(prediction_avg)\n",
    "                prediction_original = prediction_original[0]\n",
    "\n",
    "\n",
    "                if configuration_show_steps and step % 10 == 0 and step != 0:\n",
    "                    print(f\"Step {step} Target : {self.desired}, Prediction : {prediction_original}\")\n",
    "\n",
    "                self.stochastic_chosen.append(chosen)    \n",
    "                self.stochastic_predictions.append(prediction_original)\n",
    "                self.stochastic_configurations.append(configuration)\n",
    "\n",
    "                if abs(prediction_avg - y_prime) < tolerance: break\n",
    "            best = np.argsort(np.array(self.stochastic_predictions)-self.desired)[0]\n",
    "\n",
    "            self.stochastic_best_position = self.stochastic_configurations[best]\n",
    "            self.stochastic_best_prediction = self.stochastic_predictions[best]\n",
    "\n",
    "            return self.stochastic_configurations, self.stochastic_predictions, self.stochastic_best_position, self.stochastic_best_prediction\n",
    "\n",
    "\n",
    "        def best_one(self, starting_point = np.array(list(constraints.values()))[:,5].tolist(), escape = True) :\n",
    "            y_prime = self.desired / (target.max[0] - target.min[0])\n",
    "            tolerance = self.tolerance\n",
    "\n",
    "            x_i = [starting_point[i] / (feature.max[i] - feature.min[i]) for i in range(input_size)]  \n",
    "            x_prime = torch.tensor([x_i[i] - (x_i[i] % self.unit_by_feature[i]) for i in range(len(self.unit_by_feature))], \n",
    "                                   dtype = dtype)\n",
    "\n",
    "            self.best_one_chosen = []\n",
    "            self.best_one_predictions = []\n",
    "            self.best_one_configurations = []\n",
    "\n",
    "            avoid = []\n",
    "            memory = []\n",
    "            memory_size = 5\n",
    "            previous = None\n",
    "            for step in range(self.steps):\n",
    "                configuration = feature.denormalize(x_prime)\n",
    "                predictions, gradients = self.predict_global(self.models, x = x_prime, y_prime = y_prime)\n",
    "                prediction_avg = sum(predictions)/len(predictions) \n",
    "                gradient_avg = sum(gradients)/len(gradients)\n",
    "\n",
    "                if escape :\n",
    "                    candidates = [i for i in np.argsort(abs(gradient_avg))[::-1] if i not in avoid]\n",
    "                else :\n",
    "                    candidates = np.argsort(abs(gradient_avg))[::-1]\n",
    "                    #[::-1]\n",
    "                chosen = candidates[0]\n",
    "\n",
    "                adjustment = list(np.repeat(0,len(self.unit_by_feature)))\n",
    "\n",
    "                if gradient_avg[chosen] >= 0: adjustment[chosen] += self.unit_by_feature[chosen]\n",
    "                else: adjustment[chosen] -= self.unit_by_feature[chosen]\n",
    "                adjustment = np.array(adjustment)\n",
    "\n",
    "                if prediction_avg > y_prime: x_prime -= adjustment \n",
    "                elif prediction_avg < y_prime: x_prime += adjustment\n",
    "                else: pass\n",
    "                x_prime = super().bounding(x_prime)\n",
    "\n",
    "                prediction_original = target.denormalize(prediction_avg)\n",
    "                prediction_original = prediction_original[0]\n",
    "\n",
    "\n",
    "                if configuration_show_steps and step % 10 == 0 and step != 0:\n",
    "                    print(f\"Step {step} Target : {self.desired}, Prediction : {prediction_original}\")\n",
    "\n",
    "                self.best_one_chosen.append(chosen)    \n",
    "                self.best_one_predictions.append(prediction_original)\n",
    "                self.best_one_configurations.append(configuration)\n",
    "                memory.append(prediction_original)\n",
    "                if len(memory) > memory_size : memory = memory[len(memory)-memory_size:]\n",
    "                if len(memory) == 5 and len(set(memory)) < 3  and previous == chosen: avoid.append(chosen)\n",
    "\n",
    "                if abs(prediction_avg - y_prime) < tolerance: break\n",
    "                if escape and len(avoid) == input_size : break\n",
    "                previous = chosen\n",
    "            best = np.argsort(np.array(self.best_one_predictions)-self.desired)[0]\n",
    "\n",
    "            self.best_one_best_position = self.best_one_configurations[best]\n",
    "            self.best_one_best_prediction = self.best_one_predictions[best]\n",
    "\n",
    "            return self.best_one_configurations, self.best_one_predictions, self.best_one_best_position, self.best_one_best_prediction\n",
    "\n",
    "    target = MinMaxScaling(data['Target'])\n",
    "    feature = MinMaxScaling(data[[column for column in data.columns if column != 'Target']])\n",
    "    if models == None:\n",
    "        models = []\n",
    "        for model in model_list:\n",
    "            x = modeller(model)\n",
    "            models.append(x)\n",
    "        models, training_losses = train(feature.data, target.data)\n",
    "    else : training_losses = []\n",
    "        \n",
    "    configurations, predictions, best_config, best_pred = None, None, None, None\n",
    "    if mode == 'global' :\n",
    "        G = GlobalMode(desired = desired, models = models, modeling = modeling, strategy = strategy)\n",
    "        if strategy == 'beam':\n",
    "            configurations, predictions, best_config, best_pred = G.beam(starting_point = starting_point,\n",
    "                                                                 beam_width = beam_width)\n",
    "            \n",
    "        elif strategy == 'stochastic':\n",
    "            configurations, predictions, best_config, best_pred = G.stochastic(starting_point = starting_point,\n",
    "                                                                 num_candidates = num_candidates)\n",
    "            \n",
    "        elif strategy == 'best_one':\n",
    "             configurations, predictions, best_config, best_pred = G.best_one(starting_point = starting_point, \n",
    "                                                                              escape = escape)\n",
    "        \n",
    "    elif mode == 'local':\n",
    "        L = LocalMode(desired = desired, models = models, modeling = modeling, strategy = strategy)\n",
    "        if strategy == 'exhaustive':\n",
    "            configurations, predictions, best_config, best_pred = L.exhaustive(starting_point = starting_point,\n",
    "                                                                                alternative = alternative, top_k = top_k)\n",
    "        elif strategy == 'manual' :\n",
    "            configurations, predictions, best_config, best_pred = L.manual(starting_point = starting_point, index = index, up = up)\n",
    "        \n",
    "        \n",
    "    return models, training_losses, configurations, predictions, best_config, best_pred\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "[입력]\n",
    "\n",
    "    (pd.DataFrame) data\n",
    "    (list) models (모델 재이용이 아닌 경우 : None, 재이용하는 경우 리스트 안에 model들 담아서)\n",
    "    (list) model_list  (e.g., model_list = ['MLP()',\"ML_XGBoost()\",'MLP(n_layers = 2)', 'MLP(n_layers = 3)', \n",
    "                       \"ML_XGBoost()\",\"ML_LinearRegressor()\", \"ML_Ridge()\", \"ML_Lasso()\",\n",
    "                       \"ML_DecisionTreeRegressor()\",  \"ML_RandomForestRegressor()\", \"ML_GradientBoostingRegressor()\",\n",
    "                       \"ML_SVR()\", \"ML_KNeighborsRegressor()\", \"ML_HuberRegressor()\", \"ML_GaussianProcessRegressor()\" ])\n",
    "                       \n",
    "    (str) mode : {'global', 'local'}\n",
    "    (str) modeling : {'single', 'averaging', 'ensemble'} # ensemble pending\n",
    "    (str) strategy : {'beam', 'stochastic', 'best_one', 'exhaustive', 'manual', 'sensitivity'} # sensitivity pending\n",
    "    \n",
    "    (list) starting_point : 시작점\n",
    "    \n",
    "    (float) desired : 찾고자하는 목표값\n",
    "    (float) tolerance : Global 모드에서 오차허용범위\n",
    "    (int) beam_width : beam 전략에서 \n",
    "    (int) num_cadidates : stochastic 전략에서 후보 개수\n",
    "    (bool) escape : best_one 전략에서 escape 옵션 사용할 것인지 여부\n",
    "    (int) top_k : exhaustive 전략에서 alternative로 keep_up_down 선택 시 몇 개 feature를 고려할 것인지\n",
    "    (str) alternative : exhaustive 전략에서 대안 {'up_down','keep_move', 'keep_up_down'}\n",
    "    (int) index : manual 전략에서 몇번쨰 feature를 움직일 것인지\n",
    "    (bool) up : manual 전략에서 feature를 올릴 것인지 (False이면 낮춤)\n",
    "\n",
    "[출력]\n",
    "    \n",
    "    (list) models\n",
    "    (list) training_losses : 회귀모델 훈련 로스\n",
    "    (list) configurations : 입력값 리스트\n",
    "    (list) predictions : 예측값 리스트\n",
    "    (list) best_config : 최고 입력값 리스트\n",
    "    (float) best_pred : 최고 예측값\n",
    "    \n",
    "                       \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./uploads/24-learning.csv\").drop_duplicates()\n",
    "model_list = ['MLP()',\"ML_XGBoost()\"]\n",
    "starting_point = [150, 25, 40, 1, 120, 250, 10, 25, 25, 900, 0.25, 2, 100, 1800, 2000]\n",
    "\n",
    "models = None\n",
    "\n",
    "desired = 555\n",
    "mode = 'local'\n",
    "modeling = 'averaging'\n",
    "strategy = 'exhaustive'\n",
    "tolerance = 1\n",
    "beam_width = 5\n",
    "num_candidates = 5\n",
    "escape = True\n",
    "top_k = 2\n",
    "index = 0\n",
    "up = True\n",
    "alternative = 'keep_move'\n",
    "\n",
    "models, training_losses, configurations, predictions, best_config, best_pred= run(data = data, models = models,\n",
    "                                                                                  model_list = model_list, desired = desired,\n",
    "                                                                                  starting_point = starting_point, \n",
    "                                                                                  mode = mode, modeling = modeling,\n",
    "                                                                                  strategy = strategy, tolerance = tolerance, \n",
    "                                                                                  beam_width = beam_width,\n",
    "                                                                                  num_cadidates = num_candidates, escape = escape, \n",
    "                                                                                  top_k = top_k, index = index,\n",
    "                                                                                  up = up, alternative = alternative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c277a-d44f-4402-9cef-f1d9182e875e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
