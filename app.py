import firebase_admin
from firebase_admin import credentials, firestore, auth as firebase_auth
from flask import Flask, request, jsonify, send_from_directory, make_response,session
from functools import wraps
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import os, datetime, random, json, shutil, threading
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import random
from sklearn.preprocessing import MinMaxScaler
import math
import pandas as pd
import mimetypes
import pickle
import json
from model import create_model, _train_nn
from data_preprocessing import MinMaxScaling
import joblib
# ì „ì—­ì—ì„œ PyTorchì™€ ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import time
from functools import wraps
import logging
from traking import parameter_prediction
import shutil



# ë””ë²„ê¹… ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG)
# CORS ì„¤ì •ì„ ìœ„í•´ í•„ìš”í•˜ë‹¤ë©´ ë‹¤ìŒ ì½”ë“œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
from flask_cors import CORS
import numpy as np
# Python ê¸°ë³¸ random ëª¨ë“ˆ ì‹œë“œ ì„¤ì •
random.seed(42)

# NumPy ì‹œë“œ ì„¤ì •
np.random.seed(42)
# Firebase Admin SDK ì´ˆê¸°í™”
cred = credentials.Certificate("./serviceAccountKey.json")  # JSON í‚¤ íŒŒì¼ ê²½ë¡œ
firebase_admin.initialize_app(cred)
db = firestore.client()

# PyTorch ì‹œë“œ ì„¤ì • (CPU ë° GPU)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)  # ëª¨ë“  GPUì— ì ìš©
app = Flask(__name__)
app.secret_key = "super_secret_key_123!"  # âœ… ë¹„ë°€ í‚¤ ì„¤ì • (ì¤‘ìš”)
import os
app.secret_key = os.getenv("FLASK_SECRET_KEY", "fallback_secret_key")

# ğŸ”¥ ì—¬ê¸°ì— ì„¸ì…˜ ì¿ í‚¤ ê´€ë ¨ ì„¤ì •ì„ ì¶”ê°€ ğŸ”¥
app.config.update(
    SESSION_COOKIE_SAMESITE="None",  # í¬ë¡œìŠ¤ ì˜¤ë¦¬ì§„ì—ì„œ ì¿ í‚¤ í—ˆìš©
    SESSION_COOKIE_SECURE=True,      # ê°œë°œ í™˜ê²½ì—ì„œëŠ” False, ì‹¤ì œ ë°°í¬(HTTPS) ì‹œì—ëŠ” Trueë¡œ ë³€ê²½
    SESSION_COOKIE_DOMAIN=None,  # ë„ë©”ì¸ ì„¤ì • ì¶”ê°€
    SESSION_COOKIE_PATH='/'      # ê²½ë¡œ ì„¤ì • ì¶”ê°€
)

# CORS(app)
# íŠ¹ì • Origin í—ˆìš©
CORS(app, supports_credentials=True,resources={r"/*": {"origins": ["http://localhost:5173", "http://127.0.0.1:5173","http://localhost:5173/", "http://127.0.0.1:5173/"]}})

# ì—…ë¡œë“œ ë° ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
mimetypes.init()
mimetypes.add_type('application/javascript', '.js', strict=True)

# ì „ì—­ íŒŒì¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  ì‚¬ìš©ìê°€ ê³µìœ í•˜ëŠ” í´ë”ëŠ” ìµœì†Œí•œ íšŒì›ì •ë³´ ê´€ë¦¬ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©)
USERS_BASE = os.path.join(os.getcwd(), "users")  # ì‚¬ìš©ìë³„ í´ë”ëŠ” ì´ ì•„ë˜ì— ìƒì„±ë¨
os.makedirs(USERS_BASE, exist_ok=True)

# ë™ì‹œì„± ì²˜ë¦¬ë¥¼ ìœ„í•œ ê¸€ë¡œë²Œ ë½
file_lock = threading.Lock()
# ----------------- ì‚¬ìš©ìë³„ í´ë” ê´€ë ¨ í—¬í¼ í•¨ìˆ˜ -----------------
def get_user_id():
    user_id = session.get("user_id")
    if not user_id:
        raise Exception("ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    return user_id

def get_user_base_folder():
    """ë¡œê·¸ì¸í•œ ì‚¬ìš©ìì˜ ê¸°ë³¸ í´ë” (ì˜ˆ: users/user001) ë°˜í™˜"""
    user_id = get_user_id()
    base_folder = os.path.join(USERS_BASE, user_id)
    os.makedirs(base_folder, exist_ok=True)
    return base_folder

def get_user_upload_folder():
    base = get_user_base_folder()
    folder = os.path.join(base, "upload")
    os.makedirs(folder, exist_ok=True)
    return folder

def get_user_model_folder():
    base = get_user_base_folder()
    folder = os.path.join(base, "model")
    os.makedirs(folder, exist_ok=True)
    return folder

def get_user_output_folder():
    base = get_user_base_folder()
    folder = os.path.join(base, "output")
    os.makedirs(folder, exist_ok=True)
    return folder

def get_user_metadata_folder():
    base = get_user_base_folder()
    folder = os.path.join(base, "metadata")
    os.makedirs(folder, exist_ok=True)
    return folder


# ----------------- ë¡œê·¸ì¸ í•„ìš” ë°ì½”ë ˆì´í„° -----------------
def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if "user_id" not in session:
            return jsonify({'status': 'error', 'message': 'ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.'}), 401
        return f(*args, **kwargs)
    return decorated_function


# ----------------- ê³ ìœ  user_id(ìˆ«ì) ìë™ ìƒì„± í•¨ìˆ˜ -----------------
def generate_user_id():
    """
    Firestoreì˜ Counters ì»¬ë ‰ì…˜ ë‚´ 'user_counter' ë¬¸ì„œë¥¼ ì´ìš©í•˜ì—¬ ê³ ìœ í•œ user_id(ìˆ«ì)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    counter_ref = db.collection("Counters").document("user_counter")
    transaction = db.transaction()

    @firestore.transactional
    def update_counter(transaction, ref):
        # transaction.get(ref)ëŠ” generator ê°ì²´ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        snapshot = next(transaction.get(ref), None)
        if snapshot and snapshot.exists:
            data = snapshot.to_dict() or {}
            last_id = data.get("last_id", 0)
            new_id = last_id + 1
            transaction.update(ref, {"last_id": new_id})
        else:
            new_id = 1
            transaction.set(ref, {"last_id": new_id})
        return new_id

    new_number = update_counter(transaction, counter_ref)
    return new_number  # ìˆ«ìí˜• ê°’ ë°˜í™˜



# ----------------- íšŒì›ê°€ì… API -----------------
@app.route('/api/register', methods=['POST'])
def register():
    data = request.get_json()
    required_fields = ["ID", "PW", "department", "email", "phone", "user_name"]
    for field in required_fields:
        if field not in data:
            return jsonify({"status": "error", "message": f"{field} í•„ë“œëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤."}), 400

    try:
        account_id = data["ID"]

        # ì¤‘ë³µ ID í™•ì¸
        user_ref = db.collection("User").document(account_id)
        if user_ref.get().exists:
            return jsonify({"status": "error", "message": "ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ IDì…ë‹ˆë‹¤."}), 400

        # ê³ ìœ í•œ user_id ìƒì„±
        numeric_id = generate_user_id()

        # ì‚¬ìš©ì í´ë” ê²½ë¡œ ì„¤ì •
        base_path = f"./users/{account_id}"
        metadata_path_str = f"{base_path}/metadata"
        model_path_str    = f"{base_path}/model"
        output_path_str   = f"{base_path}/output"
        upload_path_str   = f"{base_path}/upload"

        # User ë¬¸ì„œì— ì €ì¥í•  ë°ì´í„°
        user_doc = {
            "ID": account_id,
            "PW": data["PW"],
            "department": data["department"],
            "email": data["email"],
            "phone": data["phone"],
            "user_id": numeric_id,
            "user_name": data["user_name"],
            "User_Profile": {               # â­ í•˜ìœ„ ì»¬ë ‰ì…˜ì´ ì•„ë‹Œ í•„ë“œë¡œ ì €ì¥
                "RANK": 0,
                "metadata": metadata_path_str,
                "model": model_path_str,
                "output": output_path_str,
                "upload": upload_path_str
            }
        }

        # Firestoreì— ì €ì¥
        user_ref.set(user_doc)

        # ì„œë²„ì— ì‚¬ìš©ì í´ë” ìƒì„±
        os.makedirs(base_path, exist_ok=True)
        os.makedirs(f"{base_path}/metadata", exist_ok=True)
        os.makedirs(f"{base_path}/model", exist_ok=True)
        os.makedirs(f"{base_path}/output", exist_ok=True)
        os.makedirs(f"{base_path}/upload", exist_ok=True)

        return jsonify({
            "status": "success",
            "message": "íšŒì›ê°€ì… ì„±ê³µ",
            "user": user_doc
        }), 200

    except Exception as e:
        logging.error(str(e))
        return jsonify({"status": "error", "message": str(e)}), 500

# ----------------- ë¡œê·¸ì¸ API -----------------
@app.route('/api/login', methods=['POST'])
def login():
    data = request.get_json()
    if "ID" not in data or "PW" not in data:
        return jsonify({"status": "error", "message": "IDì™€ PWê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
    try:
        user_ref = db.collection("User").document(data["ID"])
        user_doc = user_ref.get().to_dict()
        if not user_doc:
            return jsonify({"status": "error", "message": "ì‚¬ìš©ìê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}), 404
        if user_doc["PW"] != data["PW"]:
            return jsonify({"status": "error", "message": "ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤."}), 401
        session.permanent = True
        session["user_id"] = data["ID"]
        response = jsonify({"status": "success", "message": "ë¡œê·¸ì¸ ì„±ê³µ", "user": user_doc})

        response.set_cookie('session', session.get('user_id'), 
                       secure=True, 
                       samesite=None, 
                       httponly=True)
        print("Current session:", session)  # ì„¸ì…˜ ìƒíƒœ ë¡œê¹…
        print("Cookies:", request.cookies)  # ì¿ í‚¤ ìƒíƒœ ë¡œê¹…

        return response, 200
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

# ----------------- ë¡œê·¸ì•„ì›ƒ API -----------------
@app.route('/api/logout', methods=['POST'])
@login_required
def logout():
    session.pop("user_id", None)
    return jsonify({"status": "success", "message": "ë¡œê·¸ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤."}), 200
# ----------------- ë¡œê·¸ì¸ëœ ì‚¬ìš©ìì˜ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸° API -----------------
@app.route('/api/get_user', methods=['GET'])
@login_required
def get_user():
    print("Current session:", session)  # ì„¸ì…˜ ìƒíƒœ ë¡œê¹…
    print("Cookies:", request.cookies)  # ì¿ í‚¤ ìƒíƒœ ë¡œê¹…
    if "user_id" not in session:
        return jsonify({"status": "error", "message": "ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 401
    try:
        user_id = session["user_id"]
        user_doc = db.collection("User").document(user_id).get().to_dict()
        if not user_doc:
            return jsonify({"status": "error", "message": "ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404

        # ë¹„ë°€ë²ˆí˜¸ ì œê±°
        user_doc.pop("PW", None)

        # User_Profileì€ ì´ì œ í•„ë“œì´ë¯€ë¡œ ì§ì ‘ ì ‘ê·¼
        profile_data = user_doc.get("User_Profile", {})
        
        return jsonify({"status": "success", "user": user_doc, "profile": profile_data}), 200
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

    
# ----------------- íšŒì›ì •ë³´ ìˆ˜ì • API (PWë§Œ ìˆ˜ì • ê°€ëŠ¥) -----------------
@app.route('/api/update_password', methods=['POST'])
def update_password():
    if "user_id" not in session:
        return jsonify({"status": "error", "message": "ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 401
    data = request.get_json()
    if "old_password" not in data or "new_password" not in data:
        return jsonify({"status": "error", "message": "old_passwordì™€ new_passwordê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
    try:
        user_id = session["user_id"]
        user_ref = db.collection("User").document(user_id)
        user_doc = user_ref.get().to_dict()
        if not user_doc:
            return jsonify({"status": "error", "message": "ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404
        if user_doc["PW"] != data["old_password"]:
            return jsonify({"status": "error", "message": "ê¸°ì¡´ ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}), 401
        user_ref.update({"PW": data["new_password"]})
        return jsonify({"status": "success", "message": "ë¹„ë°€ë²ˆí˜¸ ìˆ˜ì • ì™„ë£Œ"}), 200
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500
# ----------------- íšŒì›íƒˆí‡´ API -----------------
@app.route('/api/delete_account', methods=['POST'])
@login_required
def delete_account():
    if "user_id" not in session:
        return jsonify({"status": "error", "message": "ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 401
    try:
        user_id = session["user_id"]
        db.collection("User").document(user_id).delete()
        # í•˜ìœ„ ì»¬ë ‰ì…˜ë„ ì‚­ì œí•´ì•¼ í•˜ëŠ” ê²½ìš° Firestoreì—ì„œëŠ” ì¼ê´„ ì‚­ì œ ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ User_Profile ë¬¸ì„œ ì‚­ì œ
        db.collection("User").document(user_id).collection("User_Profile").document("profile").delete()
        # ì„œë²„ íŒŒì¼ ì‹œìŠ¤í…œ ìƒì˜ ì‚¬ìš©ì í´ë” ì‚­ì œ (users í´ë” í•˜ìœ„)
        user_folder = f"./users/{user_id}"
        if os.path.exists(user_folder):
            shutil.rmtree(user_folder)
        session.pop("user_id", None)
        return jsonify({"status": "success", "message": "íšŒì›íƒˆí‡´ ì™„ë£Œ"}), 200
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

# ----------------- ê´€ë¦¬ììš©: ì „ì²´ ì‚¬ìš©ì ì •ë³´ í™•ì¸ API -----------------
@app.route('/api/admin/get_all_users', methods=['GET'])
@login_required
def admin_get_all_users():
    try:
        users = []
        for doc in db.collection("User").stream():
            user_data = doc.to_dict()
            user_data.pop("PW", None)
            profile_data = db.collection("User").document(doc.id).collection("User_Profile").document("profile").get().to_dict()
            users.append({"user": user_data, "profile": profile_data})
        return jsonify({"status": "success", "users": users}), 200
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

# ----------------- ê´€ë¦¬ììš©: ì‚¬ìš©ì Rank ìˆ˜ì • API -----------------
@app.route('/api/admin/update_rank', methods=['POST'])
def admin_update_rank():
    """
    ìš”ì²­ JSON ì˜ˆì‹œ:
    {
      "user_id": "user001",
      "RANK": 2
    }
    ê´€ë¦¬ìê°€ UserProfiles ì»¬ë ‰ì…˜ì—ì„œ í•´ë‹¹ ì‚¬ìš©ìì˜ RANK ê°’ì„ ìˆ˜ì •í•©ë‹ˆë‹¤.
    ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ê´€ë¦¬ì ì¸ì¦ì„ ë°˜ë“œì‹œ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    data = request.get_json()
    if "user_id" not in data or "RANK" not in data:
        return jsonify({"status": "error", "message": "user_idì™€ RANKê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
    try:
        db.collection("User").document(data["user_id"]).collection("User_Profile").document("profile").update({"RANK": data["RANK"]})
        return jsonify({"status": "success", "message": "Rank ìˆ˜ì • ì™„ë£Œ"}), 200
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

# ë¼ìš°íŠ¸ ì„¤ì •

@app.route('/')
def serve_index():
    return send_from_directory('static', 'index.html')

@app.route('/<path:path>')
def serve_static_file(path):
    return send_from_directory('static', path)

# API ì—”ë“œí¬ì¸íŠ¸
@app.before_request
def handle_options_request():
    if request.method == "OPTIONS":
        response = make_response()
        response.headers.add("Access-Control-Allow-Origin", request.headers.get("Origin"))
        response.headers.add("Access-Control-Allow-Methods", "GET, POST, OPTIONS")
        response.headers.add("Access-Control-Allow-Headers", "Content-Type, Authorization")
        response.headers.add("Access-Control-Allow-Credentials", "true")  # ì¶”ê°€
        response.headers.add("Access-Control-Max-Age", "3600")
        return response

# 0 CSV íŒŒì¼ ë° ë©”íƒ€ë°ì´í„° ì‚­ì œ
@app.route('/api/delete_csv', methods=['POST'])
@login_required
def delete_csv():
    """
    ì—…ë¡œë“œëœ CSV íŒŒì¼ê³¼ í•´ë‹¹ ë©”íƒ€ë°ì´í„°ë¥¼ ì‚­ì œí•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸.
    - ìš”ì²­ ì˜ˆ:
        {
          "filename": "example.csv"
        }
    - ì‘ë‹µ ì˜ˆ:
        {
          "status": "success",
          "message": "CSV file and metadata deleted successfully."
        }
    """
    data = request.json
    filename = data.get('filename')

    if not filename:
        return jsonify({'status': 'error', 'message': 'filename íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400

    # CSV íŒŒì¼ ë° ë©”íƒ€ë°ì´í„° ê²½ë¡œ ì§€ì •
    csv_path = os.path.join(get_user_upload_folder(), filename)
    metadata_path = os.path.join(get_user_metadata_folder(), f"{filename}_metadata.json")

    try:
        # CSV íŒŒì¼ ì‚­ì œ
        if os.path.exists(csv_path):
            os.remove(csv_path)
        else:
            return jsonify({'status': 'error', 'message': 'CSV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'}), 404

        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì‚­ì œ (ì—†ì–´ë„ ì—ëŸ¬ ì—†ì´ ì§„í–‰)
        if os.path.exists(metadata_path):
            os.remove(metadata_path)

        return jsonify({'status': 'success', 'message': 'CSV íŒŒì¼ ë° ë©”íƒ€ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.'})
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500
    
# 1 CSV íŒŒì¼ ì—…ë¡œë“œ
@app.route('/api/upload_csv', methods=['POST'])
@login_required
def upload_csv():
    if 'file' not in request.files:
        return jsonify({'status': 'error', 'message': 'íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({'status': 'error', 'message': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
    filename = file.filename
    file_path = os.path.join(get_user_upload_folder(), filename)
    file.save(file_path)
    return jsonify({'status': 'success', 'message': 'íŒŒì¼ ì—…ë¡œë“œ ì„±ê³µ', 'filename': filename})


# 2 ì—…ë¡œë“œëœ CSV íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
@app.route('/api/get_csv_files', methods=['GET'])
@login_required
def get_csv_files():
    files = os.listdir(get_user_upload_folder())
    return jsonify({'status': 'success', 'files': files})


@app.route('/api/save_csv_metadata', methods=['POST'])
@login_required
def save_csv_metadata():
    """
    ë©”íƒ€ë°ì´í„°ë¥¼ ìƒˆë¡œ ìƒì„±í•˜ê±°ë‚˜, ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ë¥¼ ê°±ì‹ (ë®ì–´ì“°ê¸°)í•˜ê¸° ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸.
    ìš”ì²­ ì˜ˆ:
        {
          "filename": "example.csv",
          "metadata": [
            {
              "column": "Temperature",
              "unit": "C",
              "min": 0,
              "max": 100,
              "data_type": "float",
              "round": 2
            },
            {
              "column": "Pressure",
              "unit": "bar",
              "min": 1,
              "max": 50,
              "data_type": "float",
              "round": 2
            }
          ]
        }
    ì‘ë‹µ ì˜ˆ:
        {
          "status": "success",
          "message": "Metadata saved/updated successfully.",
          "saved_metadata": [...]
        }
    """
    data = request.json
    print(data)
    filename = data.get('filename')
    metadata = data.get('metadata', [])

    if not filename:
        return jsonify({'status': 'error', 'message': 'filename íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400

    if not metadata:
        return jsonify({'status': 'error', 'message': 'metadataê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.'}), 400

    # ì‹¤ì œ CSV íŒŒì¼ì´ ì‚¬ìš©ì ì—…ë¡œë“œ í´ë”ì— ì¡´ì¬í•˜ëŠ”ì§€ ì²´í¬ (ì„ íƒì ìœ¼ë¡œ)
    csv_path = os.path.join(get_user_upload_folder(), filename)
    if not os.path.exists(csv_path):
        return jsonify({'status': 'error', 'message': 'í•´ë‹¹ CSV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'}), 404

    # unit, min, max ë“±ì„ float, intë¡œ ë³€í™˜
    for item in metadata:
        item['unit'] = float(item.get('unit', 0.0))
        item['min'] = float(item.get('min', 0.0))
        item['max'] = float(item.get('max', 0.0))
        item['round'] = int(item.get('round', 0))
        data_type_str = item.get('data_type', 'float').lower()
        item['data_type'] = data_type_str

    # ë©”íƒ€ë°ì´í„° ì €ì¥ ê²½ë¡œ: ë¡œê·¸ì¸í•œ ì‚¬ìš©ìì˜ ë©”íƒ€ë°ì´í„° í´ë”ë¥¼ ì‚¬ìš©
    metadata_path = os.path.join(get_user_metadata_folder(), f"{filename}_metadata.json")
    try:
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        return jsonify({
            'status': 'success',
            'message': 'Metadata saved/updated successfully.',
            'saved_metadata': metadata
        })
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500


# 3 CSV íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
@app.route('/api/get_csv_data', methods=['GET'])
@login_required
def get_csv_data():
    filename = request.args.get('filename')
    if not filename:
        return jsonify({'status': 'error', 'message': 'íŒŒì¼ëª…ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
    file_path = os.path.join(get_user_upload_folder(), filename)
    if not os.path.exists(file_path):
        return jsonify({'status': 'error', 'message': 'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404
    df = pd.read_csv(file_path)
    data_preview = df.head().to_dict(orient='records')
    columns = df.columns.tolist()
    return jsonify({'status': 'success', 'data_preview': data_preview, 'columns': columns})


# 4. í•„í„°ë§ëœ CSV ì €ì¥
@app.route('/api/save_filtered_csv', methods=['POST'])
@login_required
def save_filtered_csv():
    data = request.json
    print(f"Received data: {data}")  # ë””ë²„ê¹…ìš©
    exclude_columns = data.get('exclude_columns', [])
    new_filename = data.get('new_filename', '')
    filename = data.get('filename', '')  # JSONì—ì„œ filename ê°€ì ¸ì˜¤ê¸°

    if not new_filename:
        return jsonify({'status': 'error', 'message': 'ìƒˆ íŒŒì¼ ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.'}), 400

    if not filename:
        return jsonify({'status': 'error', 'message': 'ì›ë³¸ íŒŒì¼ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤.'}), 400

    file_path = os.path.join(get_user_upload_folder, filename)  # current_app ëŒ€ì‹  app ì‚¬ìš©
    print(f"File path: {file_path}")  # ë””ë²„ê¹…ìš©
    if not os.path.exists(file_path):
        return jsonify({'status': 'error', 'message': 'ì›ë³¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404

    # CSV íŒŒì¼ì—ì„œ ì œì™¸ëœ ì»¬ëŸ¼ì„ ì œê±°í•˜ê³  ì €ì¥
    df = pd.read_csv(file_path)
    filtered_df = df.drop(columns=exclude_columns, errors='ignore')
    new_file_path = os.path.join(get_user_upload_folder, new_filename)
    print(f"Saved filtered file to: {new_file_path}")
    filtered_df.to_csv(new_file_path, index=False)

    return jsonify({'status': 'success', 'message': f'í•„í„°ë§ëœ ë°ì´í„°ê°€ {new_filename}ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.'})

# # 6. ëª¨ë¸ ìƒì„±
@app.route('/api/save_model', methods=['POST'])
@login_required
def save_model():
    
    data = request.json
    if not data:
        return jsonify({'status': 'error', 'message': 'JSON ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400

    # ë””ë²„ê¹…ìš©: ìš”ì²­ ë°ì´í„° ì¶œë ¥
    print(f"Received data: {data}")
    model_type = data.get('model_type')
    model_name = data.get('model_name')
    model_selected = data.get('model_selected')
    # input_size = data.get('input_size', None)
    input_size = data.get('hyperparameters', {}).get('input_size', None)

    csv_filename = data.get('csv_filename')
    hyperparams = data.get('hyperparameters', {})
    target_column = "Target"
    epochs = hyperparams.get('epoch', 100) # ë””í´íŠ¸ 0.001
    user_model_folder = get_user_model_folder()
    save_dir = os.path.join(user_model_folder, model_name)
    os.makedirs(save_dir, exist_ok=True)

    val_ratio = float(data.get('val_ratio', 0.2))
    # í•„ìˆ˜ í•„ë“œ ëˆ„ë½ í™•ì¸
    if not model_type or not model_name or not model_selected or not csv_filename or not target_column:
        missing_fields = [field for field in ['model_type', 'model_name', 'model_selected', 'csv_filename', 'target_column'] if not data.get(field)]
        print(f"Missing fields: {missing_fields}")
        return jsonify({'status': 'error', 'message': f'í•„ìˆ˜ ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_fields}'}), 400
    
    # í•„ìˆ˜ ë°ì´í„° í™•ì¸
    if not model_type or not model_name or not model_selected or not csv_filename or not target_column:
        return jsonify({'status': 'error', 'message': 'í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.'}), 400
    
    # CSV íŒŒì¼ ë¡œë“œ
    csv_path = os.path.join(get_user_upload_folder(), csv_filename)
    if not os.path.exists(csv_path):
        return jsonify({'status': 'error', 'message': f"CSV íŒŒì¼ '{csv_filename}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404

    try:
        df = pd.read_csv(csv_path)
        #print(f"CSV ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°: {df.head()}")  # ë””ë²„ê¹…ìš©
    except Exception as e:
        return jsonify({'status': 'error', 'message': f"CSV íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"}), 500

    # Target ì»¬ëŸ¼ í™•ì¸
    if target_column not in df.columns:
        return jsonify({'status': 'error', 'message': f"Target ì»¬ëŸ¼ '{target_column}'ì´ CSV íŒŒì¼ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}), 400
    

    '''-----------------------------ì œì•½ ì¡°ê±´ì— ë”°ë¼ì„œ ëª¨ë¸ ìƒì„± ------------------------------'''
    # (2) ë©”íƒ€ë°ì´í„° ë¡œë“œ & constraints ìƒì„±
    metadata_path = os.path.join(get_user_metadata_folder(), f"{csv_filename}_metadata.json")
    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata_list = json.load(f)


    # -----------------------------------------------------------------------------
    # ### (A) ê° ì—´ì˜ min == maxì¸ ê²½ìš°, í•´ë‹¹ ì—´ì„ erase_colsì— ì¶”ê°€ í›„ í•™ìŠµì—ì„œ ì œì™¸
    # -----------------------------------------------------------------------------
    erase_cols = []
    for col in df.columns:
        if col == target_column:
            continue  # íƒ€ê²Ÿì—´ì€ ë¬´ì¡°ê±´ ì‚¬ìš©í•œë‹¤ëŠ” ê°€ì • (ì›í•˜ë©´ ì´ ì¡°ê±´ ì œê±°)
        if df[col].min() == df[col].max():
            erase_cols.append(col)
    print(erase_cols)
    print(df.shape)

    if erase_cols:
        print(f"[DEBUG] ë³€ë™ì´ ì—†ëŠ”(ìµœì†Œ=ìµœëŒ€) ì—´ë“¤: {erase_cols}. í•™ìŠµì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.")
        df.drop(columns=erase_cols, inplace=True)

    # (B) ë©”íƒ€ë°ì´í„° í•„í„°ë§
    # ê¸°ì¡´ metadata_listì—ì„œ, erase_colsì— í•´ë‹¹í•˜ëŠ” columnì˜ itemì€ ì œì™¸
    filtered_metadata = [
        item for item in metadata_list 
        if item['column'] not in erase_cols
    ]
    # ëª¨ë¸ ìƒì„±
    if input_size is not None:
        # userê°€ ì¤€ input_sizeì—ì„œ erase_cols ë§Œí¼ ë¹¼ì¤€ë‹¤
        new_input_size = input_size - len(erase_cols)
        if new_input_size < 1:
            raise ValueError(f"ìœ íš¨í•œ input_sizeê°€ ì•„ë‹™ë‹ˆë‹¤ (ì§€ì›Œì§„ ì»¬ëŸ¼ì´ ë„ˆë¬´ ë§ìŒ).")
        hyperparams["input_size"] = new_input_size
        input_size = new_input_size
    
    try:
        # GradientBoostingRegressor ë˜ëŠ” RandomForestRegressorì¼ ê²½ìš° ì™¸ë¶€ì—ì„œ ìƒì„±
        if model_selected == 'RandomForestRegressor':
            model = RandomForestRegressor(**hyperparams)
            print(f"ëª¨ë¸ ìƒì„± ì„±ê³µ (ì™¸ë¶€): {model}")
        elif model_selected == 'GradientBoostingRegressor':
            model = GradientBoostingRegressor(**hyperparams)
            print(f"ëª¨ë¸ ìƒì„± ì„±ê³µ (ì™¸ë¶€): {model}")
        else:
            # ê·¸ ì™¸ ëª¨ë¸ì€ ê¸°ì¡´ ë¡œì§ ìœ ì§€
            model = create_model(model_selected, input_size, hyperparams=hyperparams)
            print(f"ëª¨ë¸ ìƒì„± ì„±ê³µ: {model}")  # ë””ë²„ê¹…ìš©
            if not model:
                raise ValueError("ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")

    except Exception as e:
        return jsonify({"status": "error", "message": f"ëª¨ë¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500

    
        
    # constraints êµ¬ì„±
    constraints = {}
    for item in  filtered_metadata:
        col_name = item['column']       # "att1" ë“±
        unit_val = float(item['unit'])  # 5, 0.5 ë“±
        min_val  = float(item['min'])   # ì˜ˆ: 0
        max_val  = float(item['max'])   # ì˜ˆ: 100
        # ì„ì˜ë¡œ ê²°ì •í•œ íƒ€ì…, ë°˜ì˜¬ë¦¼ ìë¦¿ìˆ˜
        # ì‹¤ì œëŠ” itemì— "type" / "round_digits"ê°€ ìˆìœ¼ë©´ ê±°ê¸°ì„œ ê·¸ëŒ€ë¡œ ì½ì–´ë„ ë¨
        dtype_val = float if not float(unit_val).is_integer() else int
        round_digits = 2 if dtype_val == float else 0

        constraints[col_name] = [unit_val, min_val, max_val, dtype_val, round_digits]

    
    # ---------------------------
    # (3) ê²°ì¸¡ì¹˜ ì²˜ë¦¬, X / y ë¶„ë¦¬
    # ---------------------------
    df = df.fillna(0)
    X_df = df.drop(columns=[target_column])
    y_df = df[[target_column]]  # 2D
    print(X_df.shape)
    scaler_X = MinMaxScaling(data=X_df, constraints=constraints)  # dtype = torch.float32(ê¸°ë³¸ê°’)
    X_scaled = scaler_X.data.detach().numpy()  # (N, D) numpy
    scaler_y = MinMaxScaler()  
    y_scaled = scaler_y.fit_transform(y_df).flatten()
    
     # ---------------------------
    # (4) ë°ì´í„° ë¶„í• 
    # ---------------------------
    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=val_ratio, random_state=42)

    # feature, targetì„ np.array ë˜ëŠ” torch.tensor í˜•íƒœë¡œ ë³€í™˜ (ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ)
    X_train_np = X_train
    y_train_np = y_train.reshape(-1, 1)
    X_val_np = X_val
    y_val_np = y_val.reshape(-1, 1)
    print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)
    
    # ëª¨ë¸ í•™ìŠµ
    try:
        #print("ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        if isinstance(model, torch.nn.Module):
            training_losses = _train_nn(model, X_train_np, y_train_np, epochs)
        else:
            model.fit(X_train_np, y_train_np)  # ì—¬ê¸°ì„œ ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥ì„± í™•ì¸
        #print("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")
    except Exception as e:
        print(f"ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return jsonify({"status": "error", "message": f"ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500
    
    # í´ë” ìƒì„±
    
    model_path = os.path.join(save_dir, f"{model_name}.pt" if isinstance(model, torch.nn.Module) else f"{model_name}.pkl")
    modeldata_path = os.path.join(save_dir, f"{model_name}.json")
    creation_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

   # ëª¨ë¸ ì €ì¥
    try:
        print("ëª¨ë¸ ì €ì¥ ì‹œë„...")
        if isinstance(model, torch.nn.Module):
            torch.save(model.state_dict(), model_path)
        else:
            joblib.dump(model, model_path)  # ì—¬ê¸°ì„œ ì—ëŸ¬ ë°œìƒ ì‹œ ì¶”ì 
        print("ëª¨ë¸ ì €ì¥ ì™„ë£Œ.")
    except Exception as e:
        print(f"ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return jsonify({"status": "error", "message": f"ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500

    # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ê²€ì¦
    try:
        if isinstance(model, torch.nn.Module):
            # PyTorch ëª¨ë¸
            loaded_model = create_model(model_selected, input_size, hyperparams=hyperparams)  # ëª¨ë¸ êµ¬ì¡° ìƒì„±
            loaded_model.load_state_dict(torch.load(model_path))
            print(f"PyTorch ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {loaded_model}")
        else:
            # scikit-learn ëª¨ë¸
            loaded_model = joblib.load(model_path)
            print(f"scikit-learn ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {loaded_model}")
    except Exception as e:
        print(f"ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    # Metrics ê³„ì‚°
    def model_predict(model, X):
        if isinstance(model, torch.nn.Module):
            model.eval()
            with torch.no_grad():
                X_t = torch.tensor(X, dtype=torch.float32)
                # ì…ë ¥ ë°ì´í„°ê°€ 1Dì¸ ê²½ìš°, 2Dë¡œ ë³€í™˜
                if len(X_t.shape) == 1:
                    X_t = X_t.view(-1, 1)
                preds = model(X_t).detach().numpy().flatten()
            return preds
        else:
            return model.predict(X)

    train_predictions = model_predict(model, X_train_np)
    val_predictions = model_predict(model, X_val_np) if len(X_val_np) > 0 else None
    train_pred_inv = scaler_y.inverse_transform(train_predictions.reshape(-1, 1)).flatten()
    val_pred_inv = scaler_y.inverse_transform(val_predictions.reshape(-1, 1)).flatten() if val_predictions is not None else None

    train_mse = mean_squared_error(y_train_np, train_predictions)
    train_rmse = np.sqrt(train_mse)
    train_rae = rae(y_train_np, train_predictions)

    if val_predictions is not None:
        val_mse = mean_squared_error(y_val_np, val_predictions)
        val_rmse = np.sqrt(val_mse)
        val_rae = rae(y_val_np, val_predictions)
    else:
        val_mse = val_rmse = val_rae = None

    # # ë¹„ì •ê·œí™” (y ê°’ë§Œ ì—­ë³€í™˜)
    y_train_inv = scaler_y.inverse_transform(y_train_np)
    y_val_inv = scaler_y.inverse_transform(y_val_np) if y_val_np is not None else None
    # y ìì²´ë„ ì—­ì •ê·œí™”
    # y_train_inv = scaler_y.denormalize(y_train_np).flatten()
    # y_val_inv = scaler_y.denormalize(y_val_np).flatten() if y_val_np is not None else None

    if model_type =='pytorch':
        train_pred_inv = scaler_y.inverse_transform(train_predictions.reshape(-1,1)).flatten()
        val_pred_inv = scaler_y.inverse_transform(val_predictions.reshape(-1,1)).flatten() if val_predictions is not None else None
        
    if y_train_inv is not None:
        train_mse_inv = mean_squared_error(y_train_inv, train_pred_inv)
        train_rmse_inv = np.sqrt(train_mse_inv)
        train_rae_inv = rae(y_train_inv, train_pred_inv)
    else:
        train_mse_inv = train_rmse_inv = train_rae_inv = None

    if val_pred_inv is not None and y_val_inv is not None:
        val_mse_inv = mean_squared_error(y_val_inv, val_pred_inv)
        val_rmse_inv = np.sqrt(val_mse_inv)
        val_rae_inv = rae(y_val_inv, val_pred_inv)
    else:
        val_mse_inv = val_rmse_inv = val_rae_inv = None

    # ë©”íƒ€ë°ì´í„° ì €ì¥
    model_info = {
        'model_type': model_type,
        'model_name': model_name,
        'model_selected': model_selected,
        'parameters': hyperparams,
        'framework': 'pytorch' if isinstance(model, torch.nn.Module) else 'sklearn',
        'model_path': model_path,
        'input_size': input_size,
        'train_loss': train_mse,
        'csv_filename': csv_filename,
        'val_loss': val_mse if val_mse is not None else "N/A",
        'creation_time': creation_time,
        'metrics': {
            'scaled': {
                'train_mse': train_mse,
                'train_rmse': train_rmse,
                'train_rae': train_rae,
                'val_mse': val_mse,
                'val_rmse': val_rmse,
                'val_rae': val_rae
            },
            'original': {
                'train_mse': train_mse_inv,
                'train_rmse': train_rmse_inv,
                'train_rae': train_rae_inv,
                'val_mse': val_mse_inv,
                'val_rmse': val_rmse_inv,
                'val_rae': val_rae_inv
            }
        }
    }

    try:
        with open(modeldata_path, 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=4)
    except Exception as e:
        return jsonify({'status': 'error', 'message': f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500
    print(erase_cols)
    # ê²°ê³¼ ë°˜í™˜
    result = {
        'status': 'success',
        'message': f'ëª¨ë¸ {model_name} ìƒì„± ë° í•™ìŠµ ì™„ë£Œ',
        'model_name': model_name,
        'csv_filename': csv_filename,
        'creation_time': creation_time,
        'hyperparameters': hyperparams,
        'metrics': model_info['metrics'],
        'erase_col': erase_cols
    }

    return jsonify(result), 200

@app.route('/api/delete_model', methods=['POST'])
def delete_model():
    data = request.json
    model_name = data.get('model_name')

    if not model_name:
        return jsonify({'status': 'error', 'message': 'ëª¨ë¸ ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.'}), 400

    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    model_dir = os.path.join(MODEL_FOLDER, model_name)

    # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not os.path.exists(model_dir):
        return jsonify({'status': 'error', 'message': f'ëª¨ë¸ {model_name}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404

    if not os.path.isdir(model_dir):
        return jsonify({'status': 'error', 'message': f'{model_name}ì€(ëŠ”) ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤.'}), 400

    try:
        # ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ê³¼ ë””ë ‰í† ë¦¬ ì‚­ì œ
        for root, dirs, files in os.walk(model_dir, topdown=False):
            for name in files:
                file_path = os.path.join(root, name)
                try:
                    os.remove(file_path)
                except Exception as e:
                    print(f"íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file_path}, {str(e)}")
                    return jsonify({'status': 'error', 'message': f"íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file_path}"}), 500
            for name in dirs:
                dir_path = os.path.join(root, name)
                try:
                    os.rmdir(dir_path)
                except Exception as e:
                    print(f"ë””ë ‰í† ë¦¬ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {dir_path}, {str(e)}")
                    return jsonify({'status': 'error', 'message': f"ë””ë ‰í† ë¦¬ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {dir_path}"}), 500

        # ìµœì¢… ë””ë ‰í† ë¦¬ ì‚­ì œ
        os.rmdir(model_dir)

        return jsonify({'status': 'success', 'message': f'ëª¨ë¸ {model_name}ì´(ê°€) ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.'}), 200

    except Exception as e:
        print(f"ëª¨ë¸ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return jsonify({'status': 'error', 'message': f'ëª¨ë¸ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}), 500
    
@app.route('/api/get_models', methods=['GET'])
@login_required
def get_models():
    models = []
    MODEL_FOLDER = get_user_model_folder()
    if not os.path.exists(MODEL_FOLDER):
        os.makedirs(MODEL_FOLDER, exist_ok=True)

    for model_name in os.listdir(MODEL_FOLDER):
        model_dir = os.path.join(MODEL_FOLDER, model_name)
        metadata_path = os.path.join(model_dir, f"{model_name}.json")

        if os.path.exists(metadata_path):
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)

                    # í•„ìš”í•œ ë°ì´í„° ì¶”ì¶œ
                    model_info = {
                        'model_name': metadata.get('model_name'),
                        'framework': metadata.get('framework'),
                        'model_selected': metadata.get('model_selected'),
                        'input_size': metadata.get('input_size'),
                        'train_loss': metadata.get('train_loss'),
                        'val_loss': metadata.get('val_loss'),
                        'creation_time': metadata.get('creation_time'),  # ë‚ ì§œ ì •ë³´ ì¶”ê°€
                        'parameters': metadata.get('parameters'),  # íŒŒë¼ë¯¸í„° ì •ë³´ ì¶”ê°€
                        'csv_filename': metadata.get('csv_filename'),
                    }

                    # ìŠ¤ì¼€ì¼ë˜ì§€ ì•Šì€ ë©”íŠ¸ë¦­ ì¶”ê°€
                    original_metrics = metadata.get('metrics', {}).get('original', {})
                    model_info.update({
                        'val_rae_original': original_metrics.get('val_rae'),
                        'val_rmse_original': original_metrics.get('val_rmse'),
                    })

                    models.append(model_info)

            except Exception as e:
                print(f"ëª¨ë¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
                continue

    return jsonify({'status': 'success', 'models': models})


'''---------------------------------------------ëª¨ë¸í•™ìŠµ----------------------------------------------'''
# ëª¨ë¸ ì—…ë¡œë“œ API
@app.route('/api/upload_model', methods=['POST'])
@login_required
def upload_model():
    MODEL_FOLDER = get_user_model_folder()
    if 'model_file' not in request.files or 'model_name' not in request.form:
        return jsonify({'status': 'error', 'message': 'ëª¨ë¸ íŒŒì¼ê³¼ ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.'}), 400

    model_file = request.files['model_file']
    model_name = request.form['model_name']

    if model_file.filename == '':
        return jsonify({'status': 'error', 'message': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400

    # í—ˆìš©ëœ íŒŒì¼ í™•ì¥ì í™•ì¸
    ALLOWED_EXTENSIONS = {'pkl', 'pt', 'h5'}
    if not ('.' in model_file.filename and model_file.filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS):
        return jsonify({'status': 'error', 'message': 'í—ˆìš©ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.'}), 400

    # ëª¨ë¸ íŒŒì¼ ì €ì¥
    model_path = os.path.join(MODEL_FOLDER, f"{model_name}.{model_file.filename.rsplit('.', 1)[1].lower()}")
    model_file.save(model_path)

    # ëª¨ë¸ ì •ë³´ ì €ì¥
    model_info = {
        'model_name': model_name,
        'file_path': model_path,
        'type': 'uploaded'
    }
    metadata_path = os.path.join(MODEL_FOLDER, f"{model_name}.json")
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(model_info, f, ensure_ascii=False, indent=4)

    return jsonify({'status': 'success', 'message': f'ëª¨ë¸ {model_name}ì´(ê°€) ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.'})
def debug_training_results():
    results = []
    OUTPUTS_FOLDER = get_user_output_folder()
    if not os.path.exists(OUTPUTS_FOLDER):
        print(f"'{OUTPUTS_FOLDER}' í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return {'status': 'error', 'message': f"'{OUTPUTS_FOLDER}' í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}

    print(f"'{OUTPUTS_FOLDER}' í´ë”ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤...")
    for folder_name in os.listdir(OUTPUTS_FOLDER):
        folder_path = os.path.join(OUTPUTS_FOLDER, folder_name)
        output_file_path = os.path.join(folder_path, f"{folder_name}_output.json")

        if os.path.exists(output_file_path):
            print(f"íŒŒì¼ ë°œê²¬: {output_file_path}")
            try:
                with open(output_file_path, 'r', encoding='utf-8') as f:
                    output_data = json.load(f)
                    best_config = output_data.get('best_config')
                    best_pred = output_data.get('best_pred')

                    if best_config is not None and best_pred is not None:
                        print(f"'{folder_name}'ì—ì„œ ì½ì€ ë°ì´í„°:")
                        print(json.dumps({
                            'folder_name': folder_name,
                            'best_config': best_config,
                            'best_pred': best_pred
                        }, indent=4))

                        results.append({
                            'folder_name': folder_name,
                            'best_config': best_config,
                            'best_pred': best_pred
                        })
                    else:
                        print(f"'{output_file_path}'ì—ì„œ 'best_config' ë˜ëŠ” 'best_pred'ê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {output_file_path}, ì˜¤ë¥˜: {e}")
        else:
            print(f"'{folder_path}' í´ë”ì— '{folder_name}_output.json' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    if not results:
        print("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("ìµœì¢… ê²°ê³¼:")
        print(json.dumps({'status': 'success', 'results': results}, indent=4))

    return {'status': 'success', 'results': results}
'''--------------------------------------------------input_prediction----------------------------------------------------------------------------'''
def process_models(models_input):
    # ë§¤í•‘ ì‚¬ì „ ì •ì˜
    mapping = {
        'MLP_1': 'MLP()',
        'MLP_2': 'MLP(n_layers = 2)',
        'MLP_3': 'MLP(n_layers = 3)',
        'LinearRegressor': 'ML_LinearRegressor()',
        'Ridge': 'ML_Ridge()',
        'Lasso': 'ML_Lasso()',
        'ElasticNet': 'ML_ElasticNet()',
        'DecisionTreeRegressor': 'ML_DecisionTreeRegressor()',
        'RandomForestRegressor': 'ML_RandomForestRegressor()',
        'GradientBoostingRegressor': 'ML_GradientBoostingRegressor()',
        'SVR': 'ML_SVR()',
        'KNeighborsRegressor': 'ML_KNeighborsRegressor()',
        'HuberRegressor': 'ML_HuberRegressor()',
        'GaussianProcessRegressor': 'ML_GaussianProcessRegressor()',
        'XGBoost': 'ML_XGBoost()'
    }

    # ë§¤í•‘ëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    mapped_models = []
    MODEL_FOLDER = get_user_model_folder()
    # ëª¨ë¸ ì²˜ë¦¬ ì‹œì‘
    for model_name in models_input:
        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
        save_dir = os.path.join(MODEL_FOLDER, model_name)
        os.makedirs(save_dir, exist_ok=True)

        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        metadata_path = os.path.join(save_dir, f"{model_name}.json")
        if not os.path.exists(metadata_path):
            print(f"Metadata for model {model_name} not found. Skipping...")
            continue

        try:
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)

            # model_selected ê°’ ê°€ì ¸ì˜¤ê¸°
            model_selected = metadata.get('model_selected')
            if model_selected and model_selected in mapping:
                # ë§¤í•‘ ê²°ê³¼ ì¶”ê°€
                mapped_models.append(mapping[model_selected])
            else:
                print(f"No valid mapping for model_selected: {model_selected}. Skipping...")
        except Exception as e:
            print(f"Error processing model {model_name}: {e}")

    return mapped_models
def convert_to_serializable(obj):
    if obj is None:
        return None  # Noneì€ JSONì—ì„œ nullë¡œ ì§ë ¬í™”ë¨
    if isinstance(obj, (np.float32, np.float64)):  # numpy float ì²˜ë¦¬
        return float(obj)
    if isinstance(obj, (np.int32, np.int64)):  # numpy int ì²˜ë¦¬
        return int(obj)
    if isinstance(obj, list):  # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        return [convert_to_serializable(item) for item in obj]
    if isinstance(obj, dict):  # ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    return obj  # ë³€í™˜ì´ í•„ìš” ì—†ëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜

@app.errorhandler(Exception)
def handle_exception(e):
    logging.error(f"An error occurred: {e}")
    return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/submit_prediction', methods=['POST'])
@login_required
def submit_prediction():
    try:
        # ìš”ì²­ì—ì„œ JSON ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        data = request.get_json()
        

        # í„°ë¯¸ë„ì— ë°›ì€ ë°ì´í„° ì¶œë ¥
        logging.debug("Received data:")
        logging.debug(json.dumps(data, indent=4, ensure_ascii=False))

        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        required_fields = ['filename', 'desire', 'save_name', 'option', 'modeling_type', 'strategy', 'starting_points', 'models']
        for field in required_fields:
            if field not in data:
                logging.error(f"Missing field: {field}")
                return jsonify({'status': 'error', 'message': f'í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {field}'}), 400
        uploads = get_user_upload_folder()
        # filenameìœ¼ë¡œ CSV ë° ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ìŒ
        data_file_path = os.path.join(uploads, data['filename'])
        if not os.path.exists(data_file_path):
            logging.error(f"Data file not found: {data['filename']}")
            return jsonify({'status': 'error', 'message': f'Data file not found: {data["filename"]}'}), 400
        METADATA_FOLDER = get_user_metadata_folder()
        # (1) **ë©”íƒ€ë°ì´í„° ë¡œë“œ**: filename ê¸°ë°˜
        metadata_path = os.path.join(METADATA_FOLDER, f"{data['filename']}_metadata.json")
        if not os.path.exists(metadata_path):
            logging.error(f"Metadata file not found for: {data['filename']}")
            return jsonify({'status': 'error', 'message': f'Metadata file not found for: {data["filename"]}'}), 400

        with open(metadata_path, 'r', encoding='utf-8') as f:
            stored_metadata = json.load(f)
        units = [item['unit'] for item in stored_metadata]
        lower_bound = [item['min'] for item in stored_metadata]
        upper_bound = [item['max'] for item in stored_metadata]
        round_values = [item['round'] for item in stored_metadata]
        data_type = [item['data_type'] for item in stored_metadata]
        # CSV -> DataFrame ë¡œë“œ
        df = pd.read_csv(data_file_path).drop_duplicates()

        # 'option' í•„ë“œ ê²€ì¦
        if data['option'] not in ['local', 'global']:
            logging.error("Invalid option value")
            return jsonify({'status': 'error', 'message': 'ì˜µì…˜ ê°’ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. "local" ë˜ëŠ” "global"ì´ì–´ì•¼ í•©ë‹ˆë‹¤.'}), 400

        # 'global' ì˜µì…˜ì¼ ê²½ìš° ì¶”ê°€ í•„ë“œ í™•ì¸
        if data['option'] == 'global':
            if 'tolerance' not in data:
                logging.error("Missing tolerance field for global option")
                return jsonify({'status': 'error', 'message': 'Global ì˜µì…˜ì—ì„œëŠ” tolerance í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400

            if data['strategy'] == 'Beam' and 'beam_width' not in data:
                logging.error("Missing beam_width field for Beam strategy")
                return jsonify({'status': 'error', 'message': 'Beam ì „ëµì—ì„œëŠ” beam_width í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400

        required_keys = ['filename', 'desire', 'save_name', 'option', 'modeling_type', 'strategy']
        for key in required_keys:
            if key not in data or data[key] is None:
                raise ValueError(f"Missing or invalid value for key: {key}")

        # ë°ì´í„° íƒ€ì… ê²€ì¦
        try:
            data['desire'] = float(data['desire'])
        except ValueError:
            logging.error("Invalid desire value: must be a float")
            return jsonify({'status': 'error', 'message': 'desire ê°’ì€ ì‹¤ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.'}), 400

        # ë°ì´í„° íŒŒì¼ ë¡œë“œ
        data_file_path = os.path.join(uploads, data['filename'])
        if not os.path.exists(data_file_path):
            logging.error(f"Data file not found: {data['filename']}")
            return jsonify({'status': 'error', 'message': f'Data file not found: {data["filename"]}'}), 400

        df = pd.read_csv(data_file_path).drop_duplicates()
        print(df.shape)
        # model_list = process_models(data['models'])
        
        models = None
        mode = data['option'].lower()
        desired = int(data['desire'])
        modeling = data['modeling_type'].lower()
        strategy = data['strategy'].lower()
        tolerance = data.get('tolerance', None)
        beam_width = data.get('beam_width', None)
        num_candidates = data.get('num_candidates', None)
        
        top_k = data.get('top_k', 2)
        index = data.get('index', 0)
        converted_values = {}
        params = ['tolerance', 'beam_width', 'num_candidates', 'top_k', 'index']
        for param in params:
            value = data.get(param, None)
            try:
                converted_values[param] = int(value) if value is not None else None
            except ValueError:
                converted_values[param] = None

        tolerance = converted_values['tolerance']
        beam_width = converted_values['beam_width']
        num_candidates = converted_values['num_candidates']
        top_k = converted_values['top_k']
        index = converted_values['index']
        print(f"num_candidates = {num_candidates}")
        escape = data.get('escape', True)
        up = data.get('up', True)
        # ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ ê²½ìš° ì²˜ë¦¬
        if isinstance(escape, str):
            escape = escape.lower() == 'true'
        if isinstance(up, str):
            up = escape.lower() == 'true'
        alternative = data.get('alternative', 'keep_move')
        # ============================
        # ê¸°ì¡´ ì œì•½ì¡°ê±´ ê°€ì ¸ì˜¤ê¸°
        # ============================
        metadata_path = os.path.join(METADATA_FOLDER, f"{data['filename']}_metadata.json")

        if not os.path.exists(metadata_path):
            raise FileNotFoundError(f"Metadata file not found: {metadata_path}")
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata_list = json.load(f)
        # -----------------------------------------------------------------------------
        # ### (A) ê° ì—´ì˜ min == maxì¸ ê²½ìš°, í•´ë‹¹ ì—´ì„ erase_colsì— ì¶”ê°€ í›„ í•™ìŠµì—ì„œ ì œì™¸
        # -----------------------------------------------------------------------------
        erase_cols = []
        erase_indices = []  # ì‚­ì œí•  ì»¬ëŸ¼ë“¤ì˜ ì¸ë±ìŠ¤ ì €ì¥
        feature_cols = [col for col in df.columns if col != 'Target']

        for idx, col in enumerate(feature_cols):
            if df[col].min() == df[col].max():
                erase_cols.append(col)
                erase_indices.append(idx)

        if erase_cols:
            print(f"[DEBUG] ë³€ë™ì´ ì—†ëŠ”(ìµœì†Œ=ìµœëŒ€) ì—´ë“¤: {erase_cols}. í•™ìŠµì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.")
            df.drop(columns=erase_cols, inplace=True)

        # (B) ë©”íƒ€ë°ì´í„° í•„í„°ë§
        # ê¸°ì¡´ metadata_listì—ì„œ, erase_colsì— í•´ë‹¹í•˜ëŠ” columnì˜ itemì€ ì œì™¸
        filtered_metadata = [
            item for item in metadata_list 
            if item['column'] not in erase_cols
        ]
        # (C) erase_indicesë¥¼ ì‚¬ìš©í•´ ë¦¬ìŠ¤íŠ¸ë“¤ í•„í„°ë§
        def filter_by_indices(original_list, indices_to_remove):
            """indices_to_removeì— í•´ë‹¹í•˜ëŠ” ì›ì†Œë¥¼ original_listì—ì„œ ì œê±°"""
            return [val for idx, val in enumerate(original_list) if idx not in indices_to_remove]
                # data_type, decimal_place, starting_point í•„í„°ë§
        
        
        # ë³€í™˜í•  ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
        units = []
        max_boundary = []
        min_boundary = []
        decimal_place = []
        data_type = []
        starting_point = []
        
        # ê° metadata í•­ëª©ì„ ìˆœíšŒí•˜ë©°, columnëª…ì„ keyë¡œ í•˜ì—¬ valueë¥¼ ì €ì¥
        # filtered_metadataì—ì„œ ë°°ì—´ ê°’ ì¶”ì¶œ
        for item in filtered_metadata:
            units.append(item["unit"])
            max_boundary.append(item["max"])
            min_boundary.append(item["min"])
            decimal_place.append(item["round"])
            data_type.append(item["data_type"])
        
        data_type_mapping = {
            'int': int,
            'float': float,
            'str': str,
            'bool': bool
        }
        data_type = [data_type_mapping[dt] for dt in data_type]
        
        starting_point = list(data['starting_points'].values())
        print(starting_point)
        # starting_point = [150, 25, 40, 1, 120, 250, 10, 25, 25, 900, 0.25, 2, 100, 1800, 2000]
        starting_point = filter_by_indices(starting_point, erase_indices)

        # ============================
        # ì—¬ê¸°ì„œë¶€í„° ëª¨ë¸ ìƒì„±/íŒŒë¼ë¯¸í„° ë¡œë“œ ë¡œì§
        # ============================
        MODEL_FOLDER = get_user_model_folder()
        models_list = []

        for model_name in data['models']:
            model_dir = os.path.join(MODEL_FOLDER, model_name)
            model_metadata_path = os.path.join(model_dir, f"{model_name}.json")
            model_path = os.path.join(model_dir, f"{model_name}.pkl")
            
            if not os.path.exists(metadata_path):
                logging.error(f"No metadata found for model: {model_name}")
                continue

            try:
                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                with open(model_metadata_path, 'r', encoding='utf-8') as f:
                    model_metadata = json.load(f)

                framework = model_metadata.get('framework')
                model_selected = model_metadata.get('model_selected')
                hyperparams = model_metadata.get('parameters', {})
                input_size = model_metadata.get('input_size', None)
                model_path = ""
                print(model_selected)
                # frameworkì— ë”°ë¼ model_path ì„¤ì •
                if framework == 'sklearn':
                    model_path = os.path.join(model_dir, f"{model_name}.pkl")
                elif framework == 'pytorch':
                    model_path = os.path.join(model_dir, f"{model_name}.pt")
                else:
                    logging.error(f"Unsupported framework '{framework}' for model: {model_name}")
                    continue
                if framework == 'sklearn':
                    # (1) pkl íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë°”ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
                    try:
                        with open(model_path, 'rb') as pf:
                            model = joblib.load(pf)
                        print(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ (pkl): {model}")
                    except Exception as e:
                        logging.error(f"Error while loading model '{model_name}' from pkl: {e}")
                        continue

                elif framework == 'pytorch':
                    # PyTorch ëª¨ë¸ ë¡œë“œ ë¡œì§ (ê¸°ì¡´ ê·¸ëŒ€ë¡œ ìœ ì§€)
                    model = create_model(model_selected, input_size=input_size, hyperparams=hyperparams)
                    if not model:
                        raise ValueError(f"PyTorch ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {model_name}")
                    print(f"PyTorch ëª¨ë¸ ìƒì„± ì„±ê³µ: {model}")

                    if os.path.isfile(model_path):
                        state_dict = torch.load(model_path, map_location='cpu')
                        model.load_state_dict(state_dict)
                        print(f"PyTorch ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¡œë“œ ì„±ê³µ: {model}")

                else:
                    raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤: {framework}")

                # ì™„ì„±ëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                models_list.append(model)

            except Exception as e:
                logging.error(f"Error while creating/loading model '{model_name}': {e}")
                continue

        print("ëª¨ë¸ ìƒì„±/ë¡œë”© ì™„ë£Œ:")

        # íŒŒì¼ ì´ë¦„ ìƒì„±
        save_name = data['save_name']
        outputs_dir = get_user_output_folder()
        os.makedirs(outputs_dir, exist_ok=True)

        # ì„œë¸Œ í´ë” ì„¤ì •
        subfolder_path = os.path.join(outputs_dir, save_name)

        # ë™ì¼í•œ í´ë”ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if not os.path.exists(subfolder_path):
            os.makedirs(subfolder_path)
        
        
        # alternative = 'keep_move'
        models = data['models']
        print(f'models : {models}')
        print(f'model_list{models_list}')
        # max_boundary ë° min_boundaryë¥¼ DataFrameì—ì„œ ì§ì ‘ ì¶”ì¶œ
        # ---------------------------------------min,max ê°’ì¼ì¼íˆ ì„¤ì •í•˜ëŠ”ê±° ë°©ì§€-------------------------------------------------
        upper_bound = df.max().tolist()  # ê° ì»¬ëŸ¼ì˜ ìµœëŒ€ê°’
        lower_bound = df.min().tolist()  # ê° ì»¬ëŸ¼ì˜ ìµœì†Œê°’
        
        # desired = int(desired)
        # desired = 550
        # mode = 'global'
        # modeling = 'ensemble'
        # strategy = 'stochastic'
        # tolerance = 1
        # beam_width = 5
        # num_candidates = 5
        
        if strategy == "best one":
            strategy = "best_one"
        print(f'strategy : {strategy}')
        print(f'desired : {desired}')
        print(f'desired type : {type(desired)}')
        print(f'tolerance : {tolerance}')
        print(f'beam_width : {beam_width}')
        print(f'num_candidates : {num_candidates}')
        print(f'escape : {escape}')
        # top_k = 2
        # index = 0
        # up = True
        # alternative = 'keep_move'
        pred_all = None  # ë¯¸ë¦¬ ì„ ì–¸
        # íŒŒì¼ ê²½ë¡œ ì„¤ì • (í´ë”ëª… ê¸°ë°˜ íŒŒì¼ ì´ë¦„ ìƒì„±)
        input_file_path = os.path.join(subfolder_path, f'{save_name}_input.json')
        output_file_path = os.path.join(subfolder_path, f'{save_name}_output.json')

        configurations, predictions, best_config, best_pred, pred_all  = parameter_prediction(data = df, models = models_list,
                                                          desired = desired,
                                                          starting_point = starting_point, 
                                                          mode = mode, modeling = modeling,
                                                          strategy = strategy, tolerance = tolerance, 
                                                          beam_width = beam_width,
                                                          num_candidates = num_candidates, escape = escape, 
                                                          top_k = top_k, index = index,
                                                          up = up, alternative = alternative,
                                                          unit = units,
                                                          lower_bound = lower_bound, 
                                                          upper_bound = upper_bound, 
                                                          data_type = data_type, decimal_place = decimal_place)
        
        # í˜„ì¬ ì‹œê°(ë‚ ì§œ) ì •ë³´
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        # ì…ë ¥ ë°ì´í„°ì—ë„ ë‚ ì§œì •ë³´ ì¶”ê°€
        data['timestamp'] = timestamp
        # configurations, predictions ë“±ì„ ì „ë¶€ íŒŒì´ì¬ ê¸°ë³¸ ìë£Œí˜•ìœ¼ë¡œ ë°”ê¾¸ê¸°
        configurations = convert_to_python_types(configurations)
        predictions = convert_to_python_types(predictions)
        best_config = convert_to_python_types(best_config)
        best_pred = convert_to_python_types(best_pred)
        pred_all = convert_to_python_types(pred_all)
        
        if pred_all is not None and pred_all:
            pred_all = convert_to_python_types(pred_all)

        output_data = {
            'mode': data['option'],
            'timestamp': timestamp,  # outputì—ë„ ë™ì¼ ë‚ ì§œ ì •ë³´
            'configurations': configurations,
            'predictions': predictions,
            'best_config': best_config,
            'best_pred': best_pred,
            'Target': data['desire'],
            'filename': data['filename'],
            'erase': erase_cols,
            'pred_all': pred_all
        }

        # ë°ì´í„° ì €ì¥
        with open(input_file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        with open(output_file_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=4)

        # ì„±ê³µ ì‘ë‹µ
        return jsonify({'status': 'success', 'data': output_data}), 200

    except Exception as e:
        logging.error(f"An error occurred: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500
'''---------------------------------------------Local ì¬íƒìƒ‰----------------------------------------------'''
@app.route('/api/rerun_prediction', methods=['POST'])
@login_required
def rerun_prediction():
    """
    ì´ë¯¸ ì¡´ì¬í•˜ëŠ” results í´ë”(=outputs/í´ë”ëª…)ì™€ 
    ìƒˆë¡œìš´ starting_points ë“±ì„ ë°›ì•„ì„œ ë‹¤ì‹œ parameter_prediction ë¡œì§ì„ ìˆ˜í–‰í•˜ëŠ” API
    """
    try:
        data = request.get_json()
        if 'save_name' not in data:
            return jsonify({'status': 'error', 'message': 'save_nameì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.'}), 400

        save_name = data['save_name']
        outputs_dir = 'outputs'
        subfolder_path = os.path.join(outputs_dir, save_name)

        # ê¸°ì¡´ í´ë” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.exists(subfolder_path):
            return jsonify({
                'status': 'error', 
                'message': f'í•´ë‹¹ í´ë”({save_name})ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'
            }), 400

        # ê¸°ì¡´ input.json ë¶ˆëŸ¬ì˜¤ê¸°
        input_json_path = os.path.join(subfolder_path, f"{save_name}_input.json")
        if not os.path.exists(input_json_path):
            return jsonify({
                'status': 'error', 
                'message': f'ê¸°ì¡´ input.json íŒŒì¼ì´ {save_name} í´ë” ì•ˆì— ì—†ìŠµë‹ˆë‹¤.'
            }), 400

        with open(input_json_path, 'r', encoding='utf-8') as f:
            old_input_data = json.load(f)

        # ê¸°ì¡´ input.json ë¶ˆëŸ¬ì˜¤ê¸°
        output_json_path = os.path.join(subfolder_path, f"{save_name}_output.json")
        if not os.path.exists(output_json_path):
            return jsonify({
                'status': 'error', 
                'message': f'ê¸°ì¡´ input.json íŒŒì¼ì´ {save_name} í´ë” ì•ˆì— ì—†ìŠµë‹ˆë‹¤.'
            }), 400

        with open(output_json_path, 'r', encoding='utf-8') as f:
            old_output_data = json.load(f)
        
        erase_list = old_output_data.get('erase',  [])

        # old_input_dataì— ë“¤ì–´ìˆëŠ” ê°’ê³¼ ìƒˆë¡œ ë“¤ì–´ì˜¨ dataë¥¼ í•©ì¹œë‹¤.

        # ìš°ì„ ìˆœìœ„: ìƒˆ data ê°’ > old_input_data ê°’
        # (ê¸°ë³¸ì ìœ¼ë¡œ old_input_dataë¥¼ ë³µì‚¬í•œ ë’¤, ìƒˆ dataì— ìˆëŠ” í‚¤ëŠ” ë®ì–´ì“´ë‹¤)
        combined_data = old_input_data.copy()
        
        for key, val in data.items():
            if key == 'starting_points':
                continue
            combined_data[key] = val
        
        # # ì˜ˆ: "starting_points"ë¥¼ ìƒˆë¡œ ë°›ì•˜ìœ¼ë©´ ë®ì–´ì”Œìš°ê¸°
        # #     ë§Œì•½ "desire", "strategy" ë“±ë„ ë°”ê¾¸ê³  ì‹¶ë‹¤ë©´ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ ì§„í–‰
        # # combined_data['starting_points'] = data.get('starting_points', old_input_data.get('starting_points', {}))
        # # (4-2) starting_points ì—…ë°ì´íŠ¸: ìš”ì²­ì— 'starting_points'ê°€ ìˆìœ¼ë©´ ë®ì–´ì”Œì›€
        # if 'starting_points' in data:
        #     combined_data['starting_points'] = data['starting_points']
        # # combined_data['desire'] = float(data.get('desire', old_input_data.get('desire', 0.0)))
        # # ...

        # # ì´ì œ combined_dataë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì‹œ ëª¨ë¸ ë¡œë”©, parameter_prediction ì‹¤í–‰
        # # (ì•„ë˜ ë¡œì§ì€ /api/submit_prediction ì— ìˆëŠ” ê²ƒì„ ìµœëŒ€í•œ ì¬í™œìš©)
        for key, val in data.items():
            # starting_pointsëŠ” ì•„ë˜ì—ì„œ ë³„ë„ ì²˜ë¦¬í•  ê²ƒì´ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìŠ¤í‚µ
            if key == 'starting_points':
                continue
            combined_data[key] = val

        # ì´ì œ starting_pointsì— ëŒ€í•´ì„œë§Œ ë³„ë„ ë¡œì§ ì ìš©
        if 'starting_points' in data:
            new_values = data['starting_points']  # ex) [335, 20, 85, ...]
            old_sp = old_input_data.get('starting_points', {})

            not_erased_keys = [k for k in old_sp.keys() if k not in erase_list]

            # ê¸¸ì´ í™•ì¸
            if len(not_erased_keys) != len(new_values):
                return jsonify({'status': 'error', 'message': 'ê¸¸ì´ ë¶ˆì¼ì¹˜'}), 400

            # ë§¤í•‘
            for i, k in enumerate(not_erased_keys):
                old_sp[k] = new_values[i]

            # ìµœì¢… ë°˜ì˜
            combined_data['starting_points'] = old_sp


        # 1) filename, metadata, CSV ë¡œë“œ
        filename = combined_data['filename']
        data_file_path = os.path.join('uploads', filename)
        if not os.path.exists(data_file_path):
            return jsonify({'status': 'error', 'message': f'Data file not found: {filename}'}), 400

        df = pd.read_csv(data_file_path).drop_duplicates()
        
        # 2) ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_path = os.path.join(METADATA_FOLDER, f"{filename}_metadata.json")
        if not os.path.exists(metadata_path):
            return jsonify({
                'status': 'error', 
                'message': f'Metadata file not found for: {filename}'
            }), 400

        with open(metadata_path, 'r', encoding='utf-8') as f:
            stored_metadata = json.load(f)
        units = [item['unit'] for item in stored_metadata]
        lower_bound = [item['min'] for item in stored_metadata]
        upper_bound = [item['max'] for item in stored_metadata]
        round_values = [item['round'] for item in stored_metadata]
        data_type = [item['data_type'] for item in stored_metadata]
        # (ì›í•˜ëŠ” ëŒ€ë¡œ min_boundary, max_boundary, unit ë“± ë¡œë“œ)
        # 5-4) erase_cols ì²˜ë¦¬ (ë³€ë™ì—†ëŠ” ì—´ ì œê±°)
        erase_cols = []
        erase_indices = []
        feature_cols = [col for col in df.columns if col != 'Target']
        for idx, col in enumerate(feature_cols):
            if df[col].min() == df[col].max():
                erase_cols.append(col)
                erase_indices.append(idx)
        if erase_cols:
            df.drop(columns=erase_cols, inplace=True)
        
        # 5-5) ë©”íƒ€ë°ì´í„° í•„í„°ë§
        filtered_metadata = [
            item for item in stored_metadata 
            if item['column'] not in erase_cols
        ]
        
        # 5-6) í•„í„°ë§ í›„ ë°°ì—´ë“¤ ì¬ìƒì„±
        #      submit_prediction ì½”ë“œ ì°¸ì¡°
        def filter_by_indices(original_list, indices_to_remove):
            return [val for idx, val in enumerate(original_list) if idx not in indices_to_remove]

        # ìƒˆ units, lower_bound, upper_bound, round_values, data_type, starting_points
        units = []
        max_boundary = []
        min_boundary = []
        decimal_place = []
        data_type_list = []
        
        for item in filtered_metadata:
            units.append(item["unit"])
            max_boundary.append(item["max"])
            min_boundary.append(item["min"])
            decimal_place.append(item["round"])
            data_type_list.append(item["data_type"])

        # dtype ë§¤í•‘
        data_type_mapping = {
            'int': int,
            'float': float,
            'str': str,
            'bool': bool
        }
        data_type_list = [data_type_mapping[dt] for dt in data_type_list]
        # (5-7) starting_point ì²˜ë¦¬
        #      combined_data['starting_points']ëŠ” dictë¡œ ê°€ì •
        # sp_full = list(combined_data['starting_points'].values())
        # sp_filtered = filter_by_indices(sp_full, erase_indices)
        # 5-8) ëª¨ë¸ ë¡œë“œ
        model_names = combined_data['models']  # ê¸°ì¡´ input.json ì•ˆ models
        print(model_names)
        models_list = []
        MODEL_FOLDER = 'models'
        # 3) ëª¨ë¸ ë¡œë”© ë¡œì§
        for model_name in model_names:
            model_dir = os.path.join(MODEL_FOLDER, model_name)
            model_metadata_path = os.path.join(model_dir, f"{model_name}.json")
            model_path = os.path.join(model_dir, f"{model_name}.pkl")
            
            if not os.path.exists(metadata_path):
                logging.error(f"No metadata found for model: {model_name}")
                continue

            try:
                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                with open(model_metadata_path, 'r', encoding='utf-8') as f:
                    model_metadata = json.load(f)

                framework = model_metadata.get('framework')
                model_selected = model_metadata.get('model_selected')
                hyperparams = model_metadata.get('parameters', {})
                input_size = model_metadata.get('input_size', None)
                model_path = ""
                print(model_selected)
                # frameworkì— ë”°ë¼ model_path ì„¤ì •
                if framework == 'sklearn':
                    model_path = os.path.join(model_dir, f"{model_name}.pkl")
                elif framework == 'pytorch':
                    model_path = os.path.join(model_dir, f"{model_name}.pt")
                else:
                    logging.error(f"Unsupported framework '{framework}' for model: {model_name}")
                    continue
                if framework == 'sklearn':
                    # (1) pkl íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë°”ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
                    try:
                        with open(model_path, 'rb') as pf:
                            model = joblib.load(pf)
                        print(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ (pkl): {model}")
                    except Exception as e:
                        logging.error(f"Error while loading model '{model_name}' from pkl: {e}")
                        continue

                elif framework == 'pytorch':
                    # PyTorch ëª¨ë¸ ë¡œë“œ ë¡œì§ (ê¸°ì¡´ ê·¸ëŒ€ë¡œ ìœ ì§€)
                    model = create_model(model_selected, input_size=input_size, hyperparams=hyperparams)
                    if not model:
                        raise ValueError(f"PyTorch ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {model_name}")
                    print(f"PyTorch ëª¨ë¸ ìƒì„± ì„±ê³µ: {model}")

                    if os.path.isfile(model_path):
                        state_dict = torch.load(model_path, map_location='cpu')
                        model.load_state_dict(state_dict)
                        print(f"PyTorch ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¡œë“œ ì„±ê³µ: {model}")

                else:
                    raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤: {framework}")

                # ì™„ì„±ëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                models_list.append(model)

            except Exception as e:
                logging.error(f"Error while creating/loading model '{model_name}': {e}")
                continue

        # 4) parameter_prediction ì‹¤í–‰ì— í•„ìš”í•œ ì¸ì ì…‹íŒ…
        #    (ì›í•˜ëŠ” ë¡œì§ì— ë§ê²Œ)
        
        starting_points = data['starting_points']  # ì˜ˆ: dict í˜•íƒœ
        desired = int(combined_data['desire'])
        mode = combined_data['option']  # 'local' or 'global'
        modeling = combined_data['modeling_type'].lower()
        strategy = combined_data['strategy'].lower()
        tolerance = combined_data.get('tolerance', None)
        beam_width = combined_data.get('beam_width', None)
        num_candidates = combined_data.get('num_candidates', None)
        escape = combined_data.get('escape', True)
        # ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ ê²½ìš° ì²˜ë¦¬
        if isinstance(escape, str):
            escape = escape.lower() == 'true'
        top_k = combined_data.get('top_k', 2)
        index = combined_data.get('index', 0)
        up = combined_data.get('up', True)
        alternative = combined_data.get('alternative', 'keep_move')
        # 5-10) df.max()/min() ì´ìš©í•´ì„œ boundary ì—…ë°ì´íŠ¸ (submit_predictionê³¼ ë™ì¼)
        upper_bound = df.max().tolist()
        lower_bound = df.min().tolist()
        
        # starting_pointsê°€ dictionaryì¼ ê²½ìš°, ëª¨ë¸ì— ë“¤ì–´ê°ˆ ë•Œ listë‚˜ numpy ë³€í™˜ ë“±ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
        # ì˜ˆë¥¼ ë“¤ì–´:
        # sp_list = [float(v) for v in starting_points.values()]  # ìˆœì„œì£¼ì˜

        # ì‹¤ì œ parameter_prediction í˜¸ì¶œ
        # (ì•„ë˜ëŠ” /api/submit_prediction ì˜ˆì‹œë¥¼ ê·¸ëŒ€ë¡œ ì°¨ìš©)
        converted_values = {}
        params = ['tolerance', 'beam_width', 'num_candidates', 'top_k', 'index']
        for param in params:
            value = data.get(param, None)
            try:
                converted_values[param] = int(value) if value is not None else None
            except ValueError:
                converted_values[param] = None

        tolerance = converted_values['tolerance']
        beam_width = converted_values['beam_width']
        num_candidates = converted_values['num_candidates']
        top_k = converted_values['top_k']
        index = converted_values['index']
        print(f"num_candidates = {num_candidates}")
        escape = data.get('escape', True)
        up = data.get('up', True)
        # ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ ê²½ìš° ì²˜ë¦¬
        if isinstance(escape, str):
            escape = escape.lower() == 'true'
        if isinstance(up, str):
            up = escape.lower() == 'true'
        # 5-11) parameter_prediction í˜¸ì¶œ
        print(f'model_names : {model_names}')
        print(f'df shape : {df.shape}')
        print(f'models_list : {models_list}')
        print(f'starting_points : {starting_points}')
        print(f'mode : {mode}')
        print(f'modeling : {modeling}')
        print(f'strategy : {strategy}')
        print(f'desired : {desired}')
        print(f'desired type : {type(desired)}')
        print(f'tolerance : {tolerance}')
        print(f'beam_width : {beam_width}')
        print(f'num_candidates : {num_candidates}')
        print(f'escape : {escape}')
        print(f'top_k : {top_k}')  
        print(f'index : {index}')
        configurations, predictions, best_config, best_pred, pred_all = parameter_prediction(
            data=df,
            models=models_list,
            desired=desired,
            starting_point=starting_points, 
            mode=mode,
            modeling=modeling,
            strategy=strategy,
            tolerance=tolerance, 
            beam_width=beam_width,
            num_candidates=num_candidates,
            escape=escape, 
            top_k=top_k,
            index=index,
            up=up,
            alternative=alternative,
            unit=units,
            lower_bound=lower_bound,
            upper_bound=upper_bound,
            data_type=data_type_list,
            decimal_place=decimal_place
        )

        # (6) ê²°ê³¼ ì €ì¥
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        combined_data['timestamp'] = timestamp

        # output_data ìƒì„±
        configurations = convert_to_python_types(configurations)
        predictions = convert_to_python_types(predictions)
        best_config = convert_to_python_types(best_config)
        best_pred = convert_to_python_types(best_pred)
        pred_all = convert_to_python_types(pred_all)

        output_data = {
            'mode': mode,
            'timestamp': timestamp,
            'configurations': configurations,
            'predictions': predictions,
            'best_config': best_config,
            'best_pred': best_pred,
            'Target': desired,
            'filename': filename,
            'erase': erase_cols,
            'pred_all': pred_all
        }
        print(best_config)
        # (6-1) ìƒˆë¡œìš´ input.json, output.json ì €ì¥
        new_input_path = os.path.join(subfolder_path, f"{save_name}_input.json")
        with open(new_input_path, 'w', encoding='utf-8') as f:
            json.dump(combined_data, f, ensure_ascii=False, indent=4)

        new_output_path = os.path.join(subfolder_path, f"{save_name}_output.json")
        with open(new_output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=4)

        return jsonify({'status': 'success', 'data': output_data}), 200

    except Exception as e:
        logging.error(f"An error occurred in rerun_prediction: {str(e)}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

'''---------------------------------------------í•™ìŠµ ê²°ê³¼----------------------------------------------'''
# í•™ìŠµ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
@app.route('/api/get_training_results', methods=['GET'])
@login_required
def get_training_results():
    results = []
    OUTPUTS_FOLDER = get_user_output_folder()
    if not os.path.exists(OUTPUTS_FOLDER):
        os.makedirs(OUTPUTS_FOLDER, exist_ok=True)

    for folder_name in os.listdir(OUTPUTS_FOLDER):
        folder_path = os.path.join(OUTPUTS_FOLDER, folder_name)
        input_file_path = os.path.join(folder_path, f"{folder_name}_input.json")  # <-- (1) input JSON ê²½ë¡œë„ ì¤€ë¹„
        output_file_path = os.path.join(folder_path, f"{folder_name}_output.json")
        
        if os.path.exists(output_file_path):
            try:
                with open(output_file_path, 'r', encoding='utf-8') as f:
                    output_data = json.load(f)
                    # (2) hyperparameterìš© ë°ì´í„°ë¥¼ ì €ì¥í•  ë³€ìˆ˜
                    hyperparams_data = {}

                    # (3) input.json íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
                    if os.path.exists(input_file_path):
                        try:
                            with open(input_file_path, 'r', encoding='utf-8') as fin:
                                hyperparams_data = json.load(fin)
                        except Exception as e:
                            print(f"Error reading {input_file_path}: {e}")
                            hyperparams_data = {}
                    results.append({
                        'mode': output_data.get('mode'),
                        'timestamp': output_data.get('timestamp'),
                        'target': output_data.get('Target'),
                        'folder_name': str(folder_name),
                        'predictions' : output_data.get('predictions'),
                        'configurations': output_data.get('configurations'),
                        'best_config': output_data.get('best_config'),
                        'best_pred': output_data.get('best_pred'),
                        'hyperparameter': hyperparams_data,
                        'erase': output_data.get('erase'),
                        'pred_all':output_data.get('pred_all')

                    })
            except Exception as e:
                print(f"Error reading {output_file_path}: {e}")
                continue

    return jsonify({'status': 'success', 'results': results})


# ê²°ê³¼ ì‚­ì œ ì—”ë“œí¬ì¸íŠ¸
@app.route('/api/delete_result', methods=['POST'])
@login_required
def delete_result():
    data = request.get_json()
    filename = data.get('filename')

    if not filename:
        return jsonify({'status': 'error', 'message': 'Filename is required.'}), 400

    outputs_dir = get_user_output_folder()
    folder_path = os.path.join(outputs_dir, filename)
    print(filename)
    try:
        # Ensure the folder_path is within the outputs_dir to prevent directory traversal attacks
        outputs_dir_abs = os.path.abspath(outputs_dir)
        folder_path_abs = os.path.abspath(folder_path)
        if not folder_path_abs.startswith(outputs_dir_abs):
            return jsonify({'status': 'error', 'message': 'Invalid filename.'}), 400

        if os.path.exists(folder_path):
            # Delete the directory and all its contents
            shutil.rmtree(folder_path)
            return jsonify({'status': 'success'}), 200
        else:
            return jsonify({'status': 'error', 'message': 'Folder does not exist.'}), 404

    except Exception as e:
        logging.error(f"Error deleting folder {folder_path}: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500


# ì‚¬ìš©ì ì œê³µí–ˆë˜ EarlyStopping í´ë˜ìŠ¤ ê·¸ëŒ€ë¡œ ì‚¬ìš©
class EarlyStopping:
    def __init__(self, patience=10, delta=0.1):
        self.patience = patience
        self.delta = delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.train_loss_min = float('inf')

    def __call__(self, train_loss):
        score = -train_loss
        if self.best_score is None:
            self.best_score = score
            self.train_loss_min = train_loss
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.counter = 0
            self.train_loss_min = train_loss

@login_required
def load_constraints_from_metadata(csv_filename):
    """
    csv_filename: ì˜ˆ) "mydata.csv" (í™•ì¥ì í¬í•¨)
                  -> ë©”íƒ€ë°ì´í„° íŒŒì¼ ì´ë¦„ì€ "mydata.csv_metadata.json" ì´ë¼ ê°€ì •

    ë©”íƒ€ë°ì´í„° JSON ì˜ˆì‹œ:
    [
      {
        "column": "att1",
        "unit": 5,
        "min": 0,
        "max": 100
      },
      {
        "column": "att2",
        "unit": 10,
        "min": -5,
        "max": 50
      }
    ]
    """
    METADATA_FOLDER = get_user_metadata_folder()
    # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    metadata_path = os.path.join(METADATA_FOLDER, f"{csv_filename}_metadata.json")

    if not os.path.exists(metadata_path):
        raise FileNotFoundError(f"Metadata file not found: {metadata_path}")

    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata_list = json.load(f)

    # constraintsë¥¼ ë‹´ì„ ë”•ì…”ë„ˆë¦¬
    constraints = {}

    # ì˜ˆ) round_digitsë¥¼ ì–´ë–»ê²Œ ì„¤ì •í• ì§€ ë¯¸ë¦¬ ì •í•˜ê±°ë‚˜, ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€ í•„ë“œê°€ ìˆë‹¤ë©´ ê±°ê¸°ì„œ ì½ì–´ì˜¬ ìˆ˜ë„ ìˆìŒ
    # ì—¬ê¸°ì„œëŠ” "floatí˜•ì´ë©´ round_digits=2, intí˜•ì´ë©´ round_digits=0" ê°™ì€ ì‹ìœ¼ë¡œ ë‹¨ìˆœ ì˜ˆì‹œ
    # í˜¹ì€ column ì´ë¦„ì— ë”°ë¼ ê²°ì •í•  ìˆ˜ë„ ìˆìŒ
    def infer_type_and_digits(column_name, unit_val):
        # ì˜ˆì‹œ: unitê°’ì´ ì •ìˆ˜ì´ë©´ int, ì•„ë‹ˆë©´ floatë¡œ ê°€ì •
        if float(unit_val).is_integer():
            return int, 0  # (íƒ€ì…, ë°˜ì˜¬ë¦¼ ìë¦¿ìˆ˜=0)
        else:
            return float, 2  # (íƒ€ì…, ë°˜ì˜¬ë¦¼ ìë¦¿ìˆ˜=2)

    for item in metadata_list:
        col_name = item['column']      # ì˜ˆ: "att1"
        col_unit = float(item['unit']) # ë©”íƒ€ë°ì´í„°ì— ì €ì¥ëœ unit
        col_min  = float(item['min'])
        col_max  = float(item['max'])

        # íƒ€ì…ê³¼ ë°˜ì˜¬ë¦¼ ìë¦¿ìˆ˜ë¥¼ ì„ì˜ ê·œì¹™(í˜¹ì€ ì¶”ê°€ ë©”íƒ€ë°ì´í„°)ì— ë”°ë¼ ê²°ì •
        inferred_type, round_digits = infer_type_and_digits(col_name, col_unit)

        # ì´ì œ [ë‹¨ìœ„ê°’, min, max, íƒ€ì…, ë°˜ì˜¬ë¦¼ìë¦¿ìˆ˜] í˜•íƒœë¡œ constraints êµ¬ì„±
        constraints[col_name] = [col_unit, col_min, col_max, inferred_type, round_digits]

    return constraints

# -------------------------
# ì €ì¥ëœ ë©”íƒ€ë°ì´í„° íŒŒì¼ ëª©ë¡ ì¡°íšŒ API
# -------------------------
@app.route('/api/list_csv_metadata', methods=['GET'])
@login_required
def list_csv_metadata():
    """
    ì €ì¥ëœ ë©”íƒ€ë°ì´í„° íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•˜ê¸° ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸.
    - ìš”ì²­ ì˜ˆ:
        /api/list_csv_metadata
    - ì‘ë‹µ ì˜ˆ:
        {
          "status": "success",
          "files": [
            "example.csv_metadata.json",
            "data.csv_metadata.json"
          ]
        }
    """
    METADATA_FOLDER = get_user_metadata_folder()
    try:
        metadata_files = [
            f for f in os.listdir(METADATA_FOLDER) if f.endswith('_metadata.json')
        ]
        return jsonify({
            'status': 'success',
            'files': metadata_files
        })
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500



def rae(y_true, y_pred):
        numerator = np.sum(np.abs(y_pred - y_true))
        denominator = np.sum(np.abs(y_true - np.mean(y_true)))
        return numerator / denominator if denominator != 0 else np.nan


def convert_to_python_types(obj):
    """ì…ë ¥ëœ obj(ë¦¬ìŠ¤íŠ¸, ìŠ¤ì¹¼ë¼ ë“±) ë‚´ ëª¨ë“  ì›ì†Œë¥¼ íŒŒì´ì¬ ê¸°ë³¸ ìë£Œí˜•ìœ¼ë¡œ ë³€í™˜"""
    if isinstance(obj, torch.Tensor):
        # í…ì„œë¥¼ íŒŒì´ì¬ listë¡œ
        return obj.detach().cpu().numpy().tolist()
    elif isinstance(obj, np.ndarray):
        # ë„˜íŒŒì´ ë°°ì—´ -> list
        return obj.tolist()
    elif isinstance(obj, list):
        # ë¦¬ìŠ¤íŠ¸ë¼ë©´, ë‚´ë¶€ ìš”ì†Œë¥¼ ì¬ê·€ì ìœ¼ë¡œ ë³€í™˜
        return [convert_to_python_types(o) for o in obj]
    elif isinstance(obj, (float, int, str)):
        # ê¸°ë³¸í˜•ì´ë©´ ê·¸ëŒ€ë¡œ ë¦¬í„´
        return obj
    else:
        # í˜¹ì‹œ ëª¨ë¥´ëŠ” ì¼€ì´ìŠ¤ (ex: np.float32 ë“±)ì€ float()ë¡œ ìºìŠ¤íŒ…
        try:
            return float(obj)
        except:
            # ê·¸ë˜ë„ ì•ˆ ë˜ë©´ ê·¸ëƒ¥ ë¬¸ìì—´ ì²˜ë¦¬
            return str(obj)
        
if __name__ == '__main__':
    app.run(debug=True)