{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3715ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time  \n",
    "import random\n",
    "import xgboost\n",
    "import sklearn\n",
    "from itertools import product\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8fdd6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaling:\n",
    "    \"\"\"\n",
    "    A class for normalizing and denormalizing data using Min-Max scaling.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initializes the MinMaxScaling object.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): Input data to be scaled.\n",
    "        \"\"\"\n",
    "        self.max, self.min, self.range = [], [], []\n",
    "        self.data = pd.DataFrame()\n",
    "\n",
    "        # Reshape data if necessary\n",
    "        data = data.values.reshape(-1, 1) if len(data.values.shape) == 1 else data.values\n",
    "\n",
    "        epsilon = 2  # Small adjustment to avoid division by zero\n",
    "\n",
    "        for i in range(data.shape[1]):\n",
    "            max_, min_ = max(data[:, i]), min(data[:, i])\n",
    "            if max_ == min_:\n",
    "                max_ *= epsilon\n",
    "\n",
    "            self.max.append(max_)\n",
    "            self.min.append(min_)\n",
    "            self.range.append(max_ - min_)\n",
    "\n",
    "            # Normalize the column and add to the DataFrame\n",
    "            normalized_column = data[:, i] / (max_ - min_)\n",
    "            self.data = pd.concat([self.data, pd.DataFrame(normalized_column)], axis=1)\n",
    "\n",
    "        # Convert normalized data to a torch tensor\n",
    "        self.data = torch.tensor(self.data.values, dtype = torch.float32)\n",
    "\n",
    "    def denormalize(self, data):\n",
    "        \"\"\"\n",
    "        Denormalizes data back to its original scale.\n",
    "\n",
    "        Args:\n",
    "            data (torch.Tensor or np.ndarray): Normalized data to be converted.\n",
    "\n",
    "        Returns:\n",
    "            list: Denormalized data.\n",
    "        \"\"\"\n",
    "        # Convert torch tensor to numpy array if necessary\n",
    "        data = data.detach().numpy() if isinstance(data, torch.Tensor) else data\n",
    "\n",
    "        new_data = []\n",
    "        for i, element in enumerate(data):\n",
    "            element = element * (self.max[i] - self.min[i])\n",
    "            element = round(element, np.array(list(constraints.values()))[:, 4][i])\n",
    "            new_data.append(element)\n",
    "\n",
    "        return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cb47029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data, models, desired, starting_point, mode, modeling, strategy, tolerance, beam_width,\n",
    "        num_cadidates, escape, top_k, index, up, alternative, unit, lower_bound, upper_bound, data_type, decimal_place):\n",
    "\n",
    "    configuration_patience = 10\n",
    "    configuration_patience_volume = 0.01\n",
    "    configuration_steps = 201\n",
    "    configuration_eta = 10\n",
    "    configuration_eta_decay = 0.001\n",
    "    configuration_show_steps = True\n",
    "    configuration_show_focus = False\n",
    "    configuration_tolerance = tolerance\n",
    "    configuration_retrial_threshold = 10\n",
    "    start_from_standard = False\n",
    "    start_from_random = False\n",
    "    constrain_reselection = True\n",
    "\n",
    "    if_visualize = True\n",
    "\n",
    "    seed = 2025\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    output_size = 1\n",
    "    input_size = data.values.shape[1] - output_size\n",
    "    print(\"The number of feature  : \",input_size)\n",
    "    dtype = torch.float32\n",
    "    \n",
    "    def create_constraints(unit, lower_bound, upper_bound, data_type, decimal_place):\n",
    "        attribute_names = [\n",
    "            f\"ATT{i + 1}\" for i in range(len(unit))\n",
    "        ]\n",
    "\n",
    "        constraints = {\n",
    "            name: [\n",
    "                unit[i],\n",
    "                lower_bound[i],\n",
    "                upper_bound[i],\n",
    "                data_type[i],\n",
    "                decimal_place[i]\n",
    "            ]\n",
    "            for i, name in enumerate(attribute_names)\n",
    "        }\n",
    "\n",
    "        return constraints    \n",
    "    \n",
    "    constraints = create_constraints(unit = unit, \n",
    "                                     lower_bound = lower_bound, \n",
    "                                     upper_bound = upper_bound, \n",
    "                                     data_type = data_type, \n",
    "                                     decimal_place = decimal_place)    \n",
    "    \n",
    "\n",
    "\n",
    "    class MinMaxScaling:\n",
    "        \"\"\"\n",
    "        A class for normalizing and denormalizing data using Min-Max scaling.\n",
    "        \"\"\"\n",
    "        def __init__(self, data):\n",
    "            \"\"\"\n",
    "            Initializes the MinMaxScaling object.\n",
    "\n",
    "            Args:\n",
    "                data (pd.DataFrame): Input data to be scaled.\n",
    "            \"\"\"\n",
    "            self.max, self.min, self.range = [], [], []\n",
    "            self.data = pd.DataFrame()\n",
    "\n",
    "            # Reshape data if necessary\n",
    "            data = data.values.reshape(-1, 1) if len(data.values.shape) == 1 else data.values\n",
    "\n",
    "            epsilon = 2  # Small adjustment to avoid division by zero\n",
    "\n",
    "            for i in range(data.shape[1]):\n",
    "                max_, min_ = max(data[:, i]), min(data[:, i])\n",
    "                if max_ == min_:\n",
    "                    max_ *= epsilon\n",
    "\n",
    "                self.max.append(max_)\n",
    "                self.min.append(min_)\n",
    "                self.range.append(max_ - min_)\n",
    "\n",
    "                # Normalize the column and add to the DataFrame\n",
    "                normalized_column = data[:, i] / (max_ - min_)\n",
    "                self.data = pd.concat([self.data, pd.DataFrame(normalized_column)], axis=1)\n",
    "\n",
    "            # Convert normalized data to a torch tensor\n",
    "            self.data = torch.tensor(self.data.values, dtype=dtype)\n",
    "\n",
    "        def denormalize(self, data):\n",
    "            \"\"\"\n",
    "            Denormalizes data back to its original scale.\n",
    "\n",
    "            Args:\n",
    "                data (torch.Tensor or np.ndarray): Normalized data to be converted.\n",
    "\n",
    "            Returns:\n",
    "                list: Denormalized data.\n",
    "            \"\"\"\n",
    "            # Convert torch tensor to numpy array if necessary\n",
    "            data = data.detach().numpy() if isinstance(data, torch.Tensor) else data\n",
    "\n",
    "            new_data = []\n",
    "            for i, element in enumerate(data):\n",
    "                element = element * (self.max[i] - self.min[i])\n",
    "                element = round(element, np.array(list(constraints.values()))[:, 4][i])\n",
    "                new_data.append(element)\n",
    "\n",
    "            return new_data\n",
    "\n",
    "\n",
    "    class UNIVERSE :\n",
    "        def __init__(self, constraints = constraints):\n",
    "            self.constraints = constraints\n",
    "            self.unit_by_feature = [np.array(list(constraints.values()))[:,0][i] / (feature.max[i] - feature.min[i])\n",
    "                                    for i in range(input_size)]\n",
    "            self.upper_bounds = [np.array(list(constraints.values()))[:,2][i] / (feature.max[i] - feature.min[i])  \n",
    "                                 for i in range(input_size)]\n",
    "            self.lower_bounds = [np.array(list(constraints.values()))[:,1][i] / (feature.max[i] - feature.min[i])  \n",
    "                                 for i in range(input_size)]    \n",
    "\n",
    "            self.raw_unit = np.array(list(constraints.values()))[:,0].tolist()\n",
    "            self.raw_lower = np.array(list(constraints.values()))[:,1].tolist()\n",
    "            self.raw_upper = np.array(list(constraints.values()))[:,2].tolist()\n",
    "\n",
    "        def predict(self, models, x, y_prime, modeling, fake_gradient = True):\n",
    "            predictions,gradients = [], []\n",
    "            copy = x.clone()\n",
    "            for i, model in enumerate(models):\n",
    "                x = x.clone().detach().requires_grad_(True)\n",
    "                if isinstance(model, nn.Module):\n",
    "                    prediction = model(x)\n",
    "                    loss = abs(prediction - y_prime)\n",
    "                    loss.backward()\n",
    "                    gradient = x.grad.detach().numpy()\n",
    "                    prediction = prediction.detach().numpy()\n",
    "\n",
    "                else: # ML\n",
    "                    x = x.detach().numpy().reshape(1,-1)\n",
    "                    prediction = model.predict(x)\n",
    "                    if fake_gradient :\n",
    "                        gradient = []\n",
    "                        for j in range(x.shape[1]):\n",
    "                            new_x = x.copy()\n",
    "                            new_x[0,j] += self.unit_by_feature[j]\n",
    "                            new_prediction = model.predict(new_x)\n",
    "                            slope = (new_prediction - prediction) / self.unit_by_feature[j]\n",
    "                            gradient.append(slope)\n",
    "                        gradient = np.array(gradient).reshape(-1)   \n",
    "                    else : gradient = np.repeat(0, x.shape[1])\n",
    "                x = copy.clone()\n",
    "                predictions.append(prediction)\n",
    "                gradients.append(gradient)\n",
    "\n",
    "            if modeling == 'single': return predictions[0], gradients[0], predictions\n",
    "            elif modeling == 'ensemble': return sum(predictions)/len(predictions), sum(gradients)/len(gradients), predictions  \n",
    "            else: raise Exception(f\"[modeling error] there is no {modeling}.\")\n",
    "\n",
    "\n",
    "        def bounding(self, configuration):\n",
    "            new = []\n",
    "            for k, element in enumerate(configuration):\n",
    "                element = element - (element % self.unit_by_feature[k])\n",
    "                if element >= self.upper_bounds[k] : element = self.upper_bounds[k]\n",
    "                elif element <= self.lower_bounds[k] : element = self.lower_bounds[k]\n",
    "                else: pass\n",
    "                new.append(element)        \n",
    "            configuration = torch.tensor(new, dtype = dtype)        \n",
    "            return configuration\n",
    "\n",
    "\n",
    "        def truncate(self, configuration):\n",
    "            new_configuration = []\n",
    "            for i, value in enumerate(configuration) :\n",
    "                value = value - (value % self.raw_unit[i])\n",
    "                value = value if value >= self.raw_lower[i] else self.raw_lower[i]\n",
    "                value = value if value <= self.raw_upper[i] else self.raw_upper[i]\n",
    "                new_configuration.append(value)\n",
    "           # configuration = torch.tensor(new_configuration, dtype = dtype)     \n",
    "            return configuration\n",
    "        \n",
    "    class LocalMode(UNIVERSE):\n",
    "        def __init__(self, desired, models, modeling, strategy):\n",
    "            super().__init__()\n",
    "            self.desired = desired\n",
    "            self.y_prime = self.desired / (target.max[0] - target.min[0])\n",
    "            self.models = []\n",
    "            self.modeling = modeling\n",
    "            self.strategy = strategy\n",
    "            for model in models : \n",
    "                if isinstance(model, nn.Module) : model.eval()\n",
    "                self.models.append(model)\n",
    "\n",
    "        def exhaustive(self, starting_point, top_k = 5, alternative = 'keep_move'):\n",
    "            self.starting_point = super().truncate(starting_point)\n",
    "            self.starting_point = np.array([self.starting_point[i] / (feature.max[i] - feature.min[i]) \n",
    "                                            for i in range(input_size)])\n",
    "            self.top_k = top_k\n",
    "            self.recorder = []\n",
    "\n",
    "            self.search_space, self.counter = [],[]\n",
    "            if alternative == 'keep_up_down' :\n",
    "                variables = [[0, 1, 2]] * input_size\n",
    "            else :\n",
    "                variables = [[0, 1]] * input_size\n",
    "            self.all_combinations = list(product(*variables))\n",
    "            self.adj = []\n",
    "\n",
    "            if alternative == 'keep_move' or alternative == 'keep_up_down':\n",
    "                prediction_km, gradient_km, p_all = super().predict(self.models,torch.tensor(self.starting_point, dtype = dtype), \n",
    "                                               self.y_prime, self.modeling, fake_gradient = True)\n",
    "                prediction_km = prediction_km[0] if isinstance(prediction_km,list) else prediction_km\n",
    "            for combination in self.all_combinations :\n",
    "                count = combination.count(1) if alternative == 'up_down' else combination.count(0)\n",
    "                adjustment = np.repeat(0,len(self.unit_by_feature)).tolist()\n",
    "                for i, boolean in enumerate(list(combination)):\n",
    "                    if alternative == 'up_down':\n",
    "                        if boolean == 1 :   adjustment[i] = adjustment[i] + self.unit_by_feature[i]\n",
    "                        elif boolean == 0 : adjustment[i] = adjustment[i] - self.unit_by_feature[i]\n",
    "                        else: raise Exception(\"ERROR\")\n",
    "\n",
    "                    elif alternative == 'keep_move':\n",
    "                        if prediction_km > self.y_prime :                        \n",
    "                            if boolean == 1 :   \n",
    "                                if gradient_km[i] >= 0 :\n",
    "                                    adjustment[i] = adjustment[i] - self.unit_by_feature[i]\n",
    "                                else:\n",
    "                                    adjustment[i] = adjustment[i] + self.unit_by_feature[i]\n",
    "                            elif boolean == 0 : pass\n",
    "                            else: raise Exception(\"ERROR\")            \n",
    "\n",
    "                        else: \n",
    "                            if boolean == 1 :   \n",
    "                                if gradient_km[i] >= 0 :\n",
    "                                    adjustment[i] = adjustment[i] + self.unit_by_feature[i]\n",
    "                                else:\n",
    "                                    adjustment[i] = adjustment[i] - self.unit_by_feature[i]\n",
    "                            elif boolean == 0 : pass\n",
    "                            else: raise Exception(\"ERROR\")    \n",
    "\n",
    "                    elif alternative == 'keep_up_down' :\n",
    "                        important_features = np.argsort(abs(gradient_km))[::-1][:self.top_k]\n",
    "                        if i in important_features :\n",
    "                            if boolean == 2 :   adjustment[i] = adjustment[i] + self.unit_by_feature[i]\n",
    "                            elif boolean == 1 : adjustment[i] = adjustment[i] - self.unit_by_feature[i]\n",
    "                            elif boolean == 0 : pass\n",
    "                            else: raise Exception(\"ERROR\")   \n",
    "\n",
    "                self.adj.append(adjustment)\n",
    "                candidate = self.starting_point + adjustment         \n",
    "                candidate = super().bounding(candidate)\n",
    "                if str(candidate) not in self.recorder : \n",
    "                    self.search_space.append(candidate)\n",
    "                    self.counter.append(count)\n",
    "                    self.recorder.append(str(candidate))\n",
    "        #    print(len(self.search_space))\n",
    "            self.predictions = []\n",
    "            self.configurations = []\n",
    "            self.pred_all = []\n",
    "            for candidate in self.search_space :\n",
    "                prediction, _, p_all = super().predict(self.models,candidate, self.y_prime, self.modeling, fake_gradient = False)\n",
    "                prediction = target.denormalize(prediction)[0]\n",
    "                configuration = feature.denormalize(candidate)\n",
    "                self.predictions.append(prediction)\n",
    "                self.configurations.append(configuration)\n",
    "                self.pred_all.append([target.denormalize([e.item()])[0] for e in p_all])\n",
    "                \n",
    "\n",
    "            self.table = pd.DataFrame({'configurations':self.configurations,'find_dup' : self.configurations,\n",
    "                                      'predictions':self.predictions,'difference' : np.array(abs(np.array(self.predictions)-self.desired)).tolist(),\n",
    "                                      'counter':self.counter})\n",
    "            self.table['find_dup'] = self.table['find_dup'].apply(lambda x: str(x))\n",
    "            self.table = self.table[~self.table.duplicated(subset='find_dup', keep='first')]\n",
    "            self.table = self.table.drop(columns=['find_dup'])\n",
    "\n",
    "            self.table = self.table.sort_values(by='counter', ascending=False).sort_values(by='difference', ascending=True)\n",
    "            self.configurations = self.table['configurations']\n",
    "            self.predictions = self.table['predictions']\n",
    "            self.difference = self.table['difference']\n",
    "            self.counter = self.table['counter']\n",
    "            \n",
    "            configurations = self.configurations[:].values.tolist()\n",
    "            predictions = self.predictions[:].values.tolist()\n",
    "            best_config = configurations[0]\n",
    "            best_pred = predictions[0]\n",
    "\n",
    "            try: return configurations, predictions, best_config, best_pred, self.pred_all\n",
    "            except : return self.configurations[:].values.tolist(), self.predictions[:].values.tolist(), best_config, best_pred, self.pred_all\n",
    "\n",
    "        def manual(self, starting_point, index=0, up=True):\n",
    "            self.starting_point = super().truncate(starting_point)\n",
    "            self.starting_point = np.array([self.starting_point[i] / (feature.max[i] - feature.min[i]) \n",
    "                                            for i in range(input_size)])\n",
    "\n",
    "            adjustment = np.repeat(0,len(self.unit_by_feature)).tolist()\n",
    "            if up : adjustment[index] += self.unit_by_feature[index]\n",
    "            else : adjustment[index] -= self.unit_by_feature[index]\n",
    "            position = self.starting_point + adjustment         \n",
    "            position = super().bounding(position)        \n",
    "            prediction, _, p_all = super().predict(self.models,position, self.y_prime, self.modeling)\n",
    "            prediction = target.denormalize(prediction)\n",
    "            configuration = feature.denormalize(position)\n",
    "            p_all = [target.denormalize([e.item()])[0] for e in p_all]\n",
    "            return [configuration], prediction, configuration, prediction, p_all \n",
    "        \n",
    "    class GlobalMode(UNIVERSE):\n",
    "        def __init__(self, desired, models, modeling, strategy, tolerance = configuration_tolerance, steps = configuration_steps):\n",
    "            super().__init__()\n",
    "            self.desired = desired\n",
    "            self.y_prime = self.desired / (target.max[0] - target.min[0])\n",
    "            self.models = []\n",
    "            self.modeling = modeling\n",
    "            self.strategy = strategy\n",
    "            self.tolerance = tolerance / (target.max[0] - target.min[0])\n",
    "            self.steps = steps\n",
    "            for model in models : \n",
    "                if isinstance(model, nn.Module): model.train()\n",
    "                self.models.append(model)\n",
    "\n",
    "        def predict_global(self, models, x, y_prime):\n",
    "\n",
    "            predictions,gradients = [], []\n",
    "            copy = x.clone()\n",
    "            for i, model in enumerate(models):\n",
    "                x = x.clone().detach().requires_grad_(True)\n",
    "                if isinstance(model, nn.Module):\n",
    "                    prediction = model(x)\n",
    "                    loss = prediction - y_prime\n",
    "                    loss.backward()\n",
    "                    gradient = x.grad.detach().numpy()\n",
    "                    prediction = prediction.detach().numpy()\n",
    "\n",
    "                else:\n",
    "                    x = x.detach().numpy().reshape(1,-1)\n",
    "                    prediction = model.predict(x)\n",
    "                    gradient = []\n",
    "                    for j in range(x.shape[1]):\n",
    "                        new_x = x.copy()\n",
    "                        new_x[0,j] += self.unit_by_feature[j]\n",
    "                        new_prediction = model.predict(new_x)\n",
    "                        slope = (new_prediction - prediction) / self.unit_by_feature[j]\n",
    "                        gradient.append(slope)\n",
    "                    gradient = np.array(gradient).reshape(-1)   \n",
    "                x = copy.clone()\n",
    "                predictions.append(prediction)\n",
    "                gradients.append(gradient)\n",
    "            return predictions, gradients\n",
    "\n",
    "        def beam(self, beam_width, starting_point) :\n",
    "            self.beam_width = beam_width\n",
    "            y_prime = self.desired / (target.max[0] - target.min[0])\n",
    "            tolerance = self.tolerance\n",
    "            final = None\n",
    "\n",
    "            x_i = [starting_point[i] / (feature.max[i] - feature.min[i]) for i in range(input_size)]  \n",
    "            x_prime = torch.tensor([x_i[i] - (x_i[i] % self.unit_by_feature[i]) for i in range(len(self.unit_by_feature))], \n",
    "                                   dtype = dtype)\n",
    "\n",
    "            self.beam_positions, self.beam_targets = [], []\n",
    "            self.beam_positions_denorm, self.beam_targets_denorm = [], []\n",
    "\n",
    "            self.beam_history = []\n",
    "            self.previous_gradient = [[], [], [], [], []]\n",
    "            self.prediction_all = []\n",
    "\n",
    "            success = [False]\n",
    "            which = []\n",
    "            close = False\n",
    "            close_margin = 10\n",
    "            for step in range(self.steps):\n",
    "                if len(self.beam_positions) == 0 :\n",
    "                    current_positions = [x_prime.clone().detach()]\n",
    "                    for j in range(self.beam_width - 1):\n",
    "                        random_offsets = torch.tensor(np.random.uniform(-0.5, 0.5, input_size), dtype=x_prime.dtype)\n",
    "                        current_positions += [x_prime.clone().detach() + random_offsets]\n",
    "                else :\n",
    "                    current_positions = self.beam_positions[-1]\n",
    "\n",
    "                configurations = []\n",
    "                candidates = []\n",
    "                candidates_score = []\n",
    "                beam_predictions = []\n",
    "                beams = []\n",
    "                p_all = []\n",
    "                for p, current_pos in enumerate(current_positions):\n",
    "                    configuration = feature.denormalize(current_pos.clone().detach())\n",
    "                    configurations.append(configuration)\n",
    "\n",
    "                    predictions, gradients = self.predict_global(self.models, x = current_pos, y_prime = y_prime)                \n",
    "                    prediction_avg = sum(predictions)/len(predictions) ###\n",
    "                    gradient_avg = sum(gradients)/len(gradients)  \n",
    "\n",
    "                    prediction_original = prediction_avg * (target.max[0] - target.min[0])\n",
    "                    prediction_original = prediction_original[0]\n",
    "                    beam_predictions.append(prediction_original)     \n",
    "                    p_all.append([target.denormalize([e.item()])[0] for e in predictions])\n",
    "                    if abs(prediction_original - self.desired) < close_margin : close = True\n",
    "                    else : close = False\n",
    "                #    print(close)\n",
    "                    if abs(prediction_avg - y_prime) < tolerance:\n",
    "                        best_config = configuration\n",
    "                        best_pred = prediction_original\n",
    "                        success.append(True)\n",
    "\n",
    "               #     if close :\n",
    "               #         order = np.argsort(abs(gradient_avg))\n",
    "               #     else :\n",
    "               #         order = np.argsort(abs(gradient_avg))[::-1]\n",
    "               #     order = np.random.permutation(np.argsort(abs(gradient_avg))[::-1])\n",
    "                    order = np.argsort(abs(gradient_avg))[::-1]\n",
    "\n",
    "                    beam = order[:self.beam_width]\n",
    "                    \n",
    "                    for b in beam:\n",
    "\n",
    "                        adjustment = list(np.repeat(0,len(self.unit_by_feature)))\n",
    "                        if gradient_avg[b] >= 0 :\n",
    "                            adjustment[b] += self.unit_by_feature[b]\n",
    "                        else:\n",
    "                            adjustment[b] -= self.unit_by_feature[b]    \n",
    "\n",
    "\n",
    "                        adjustment = np.array(adjustment)\n",
    "                        if prediction_avg > y_prime : \n",
    "                            position = current_pos.clone().detach() - adjustment \n",
    "                        else :\n",
    "                            position = current_pos.clone().detach() + adjustment \n",
    "\n",
    "                        position = super().bounding(position)\n",
    "                        candidates.append(position)\n",
    "                        candidates_score.append(abs(gradient_avg[b]))\n",
    "\n",
    "                if step % 10 == 0 and step != 0 : print(f\"Step {step} Target : {self.desired}, Prediction : {beam_predictions}\")\n",
    "                select = np.argsort(candidates_score)[::-1][:self.beam_width]\n",
    "                new_positions = [torch.tensor(candidates[s], dtype = dtype) for s in select]\n",
    "\n",
    "                if len(beam_predictions) == 1 : beam_predictions = list(np.repeat(beam_predictions[0],self.beam_width))\n",
    "                self.beam_positions.append(new_positions)\n",
    "                self.beam_targets.append(beam_predictions)\n",
    "                self.beam_history.append(beam.tolist())\n",
    "                self.beam_positions_denorm.append(configurations) \n",
    "                self.beam_targets_denorm.append(beam_predictions)\n",
    "                self.prediction_all.append(p_all)\n",
    "                if any(success): break      \n",
    "\n",
    "            flattened_positions = list(chain.from_iterable(self.beam_positions_denorm))\n",
    "            flattened_predictions = list(chain.from_iterable(self.beam_targets_denorm))\n",
    "            best = np.argsort(abs(np.array(flattened_predictions)-self.desired))[0]\n",
    "\n",
    "            self.best_position = flattened_positions[best]\n",
    "            self.best_prediction = flattened_predictions[best]\n",
    "\n",
    "            return self.beam_positions_denorm, self.beam_targets_denorm, self.best_position, self.best_prediction, self.prediction_all\n",
    "\n",
    "\n",
    "        def stochastic(self, num_candidates = 5, starting_point = starting_point) :\n",
    "            self.num_candidates = num_candidates\n",
    "            y_prime = self.desired / (target.max[0] - target.min[0])\n",
    "            tolerance = self.tolerance\n",
    "            final = None\n",
    "\n",
    "            x_i = [starting_point[i] / (feature.max[i] - feature.min[i]) for i in range(input_size)]  \n",
    "            x_prime = torch.tensor([x_i[i] - (x_i[i] % self.unit_by_feature[i]) for i in range(len(self.unit_by_feature))], \n",
    "                                   dtype = dtype)\n",
    "\n",
    "            self.stochastic_chosen = []\n",
    "            self.stochastic_predictions = []\n",
    "            self.stochastic_configurations = []\n",
    "            self.stochastic_predictions_all = []\n",
    "            self.prediction_all = []\n",
    "\n",
    "            for step in range(self.steps):\n",
    "                configuration = feature.denormalize(x_prime)\n",
    "                predictions, gradients = self.predict_global(self.models, x = x_prime, y_prime = y_prime)\n",
    "                prediction_avg = sum(predictions)/len(predictions) \n",
    "                gradient_avg = sum(gradients)/len(gradients)\n",
    "\n",
    "                candidates = np.argsort(abs(gradient_avg))[::-1][:self.num_candidates] #[::-1]\n",
    "                chosen = random.choice(candidates)\n",
    "\n",
    "                adjustment = list(np.repeat(0,len(self.unit_by_feature)))\n",
    "\n",
    "                if gradient_avg[chosen] >= 0: adjustment[chosen] += self.unit_by_feature[chosen]\n",
    "                else: adjustment[chosen] -= self.unit_by_feature[chosen]\n",
    "                adjustment = np.array(adjustment)\n",
    "\n",
    "                if prediction_avg > y_prime: x_prime -= adjustment \n",
    "                elif prediction_avg < y_prime: x_prime += adjustment\n",
    "                else: pass\n",
    "                x_prime = super().bounding(x_prime)\n",
    "\n",
    "                prediction_original = target.denormalize(prediction_avg)\n",
    "                prediction_original = prediction_original[0]\n",
    "                \n",
    "             #   prediction_original_all = target.denormalize(predictions_all)\n",
    "             #   prediction_original_all = prediction_original_all\n",
    "                \n",
    "                if configuration_show_steps and step % 10 == 0 and step != 0:\n",
    "                    print(f\"Step {step} Target : {self.desired}, Prediction : {prediction_original}\")\n",
    "\n",
    "                self.stochastic_chosen.append(chosen)    \n",
    "                self.stochastic_predictions.append(prediction_original)\n",
    "                self.stochastic_configurations.append(configuration)\n",
    "                self.prediction_all.append([target.denormalize([e.item()])[0] for e in predictions])\n",
    "            #    self.stochastic_predictions_all.append(prediction_original_all)\n",
    "\n",
    "                if abs(prediction_avg - y_prime) < tolerance: break\n",
    "                    \n",
    "            best = np.argsort(abs(np.array(self.stochastic_predictions)-self.desired))[0]\n",
    "\n",
    "            self.stochastic_best_position = self.stochastic_configurations[best]\n",
    "            self.stochastic_best_prediction = self.stochastic_predictions[best]\n",
    "\n",
    "            return self.stochastic_configurations, self.stochastic_predictions, self.stochastic_best_position,self.stochastic_best_prediction, self.prediction_all\n",
    "\n",
    "\n",
    "        def best_one(self, starting_point, escape = True) :\n",
    "            y_prime = self.desired / (target.max[0] - target.min[0])\n",
    "            tolerance = self.tolerance\n",
    "\n",
    "            x_i = [starting_point[i] / (feature.max[i] - feature.min[i]) for i in range(input_size)]  \n",
    "            x_prime = torch.tensor([x_i[i] - (x_i[i] % self.unit_by_feature[i]) for i in range(len(self.unit_by_feature))], \n",
    "                                   dtype = dtype)\n",
    "\n",
    "            self.best_one_chosen = []\n",
    "            self.best_one_predictions = []\n",
    "            self.best_one_configurations = []\n",
    "            self.prediction_all = []\n",
    "\n",
    "            avoid = []\n",
    "            memory = []\n",
    "            memory_size = 5\n",
    "            previous = None\n",
    "            for step in range(self.steps):\n",
    "                configuration = feature.denormalize(x_prime)\n",
    "                predictions, gradients = self.predict_global(self.models, x = x_prime, y_prime = y_prime)\n",
    "                prediction_avg = sum(predictions)/len(predictions) \n",
    "                gradient_avg = sum(gradients)/len(gradients)\n",
    "\n",
    "                if escape :\n",
    "                    candidates = [i for i in np.argsort(abs(gradient_avg))[::-1] if i not in avoid]\n",
    "                else :\n",
    "                    candidates = np.argsort(abs(gradient_avg))[::-1]\n",
    "                    #[::-1]\n",
    "                chosen = candidates[0]\n",
    "\n",
    "                adjustment = list(np.repeat(0,len(self.unit_by_feature)))\n",
    "\n",
    "                if gradient_avg[chosen] >= 0: adjustment[chosen] += self.unit_by_feature[chosen]\n",
    "                else: adjustment[chosen] -= self.unit_by_feature[chosen]\n",
    "                adjustment = np.array(adjustment)\n",
    "\n",
    "                if prediction_avg > y_prime: x_prime -= adjustment \n",
    "                elif prediction_avg < y_prime: x_prime += adjustment\n",
    "                else: pass\n",
    "                x_prime = super().bounding(x_prime)\n",
    "\n",
    "                prediction_original = target.denormalize(prediction_avg)\n",
    "                prediction_original = prediction_original[0]\n",
    "\n",
    "\n",
    "                if configuration_show_steps and step % 10 == 0 and step != 0:\n",
    "                    print(f\"Step {step} Target : {self.desired}, Prediction : {prediction_original}\")\n",
    "\n",
    "                self.best_one_chosen.append(chosen)    \n",
    "                self.best_one_predictions.append(prediction_original)\n",
    "                self.best_one_configurations.append(configuration)\n",
    "                self.prediction_all.append([target.denormalize([e.item()])[0] for e in predictions])\n",
    "\n",
    "                \n",
    "                memory.append(prediction_original)\n",
    "                if len(memory) > memory_size : memory = memory[len(memory)-memory_size:]\n",
    "                if len(memory) == 5 and len(set(memory)) < 3  and previous == chosen: avoid.append(chosen)\n",
    "\n",
    "                if abs(prediction_avg - y_prime) < tolerance: break\n",
    "                if escape and len(avoid) == input_size : break\n",
    "                previous = chosen\n",
    "            best = np.argsort(abs(np.array(self.best_one_predictions)-self.desired))[0]\n",
    "\n",
    "            self.best_one_best_position = self.best_one_configurations[best]\n",
    "            self.best_one_best_prediction = self.best_one_predictions[best]\n",
    "\n",
    "            return self.best_one_configurations, self.best_one_predictions, self.best_one_best_position, self.best_one_best_prediction, self.prediction_all\n",
    "\n",
    "    target = MinMaxScaling(data['Target'])\n",
    "    feature = MinMaxScaling(data[[column for column in data.columns if column != 'Target']])\n",
    "    \n",
    "    configurations, predictions, best_config, best_pred = None, None, None, None\n",
    "    if mode == 'global' :\n",
    "        G = GlobalMode(desired = desired, models = models, modeling = modeling, strategy = strategy)\n",
    "        if strategy == 'beam':\n",
    "            configurations, predictions, best_config, best_pred, pred_all = G.beam(starting_point = starting_point,\n",
    "                                                                 beam_width = beam_width)\n",
    "            \n",
    "        elif strategy == 'stochastic':\n",
    "            configurations, predictions, best_config, best_pred, pred_all = G.stochastic(starting_point = starting_point,\n",
    "                                                                 num_candidates = num_candidates)\n",
    "            \n",
    "        elif strategy == 'best_one':\n",
    "             configurations, predictions, best_config, best_pred, pred_all = G.best_one(starting_point = starting_point, \n",
    "                                                                              escape = escape)\n",
    "        \n",
    "    elif mode == 'local':\n",
    "        L = LocalMode(desired = desired, models = models, modeling = modeling, strategy = strategy)\n",
    "        if strategy == 'exhaustive':\n",
    "            configurations, predictions, best_config, best_pred, pred_all = L.exhaustive(starting_point = starting_point,\n",
    "                                                                                alternative = alternative, top_k = top_k)\n",
    "        elif strategy == 'manual' :\n",
    "            configurations, predictions, best_config, best_pred, pred_all = L.manual(starting_point = starting_point, index = index, up = up)\n",
    "        \n",
    "    if mode == 'global' and len(predictions) > 1:\n",
    "        configurations = configurations[1:]\n",
    "        predictions = predictions[1:]\n",
    "        pred_all = pred_all[1:]\n",
    "        \n",
    "    configurations = [\n",
    "        [data_type[col](value) for col, value in enumerate(configurations[row])]\n",
    "        for row in range(len(configurations))\n",
    "    ]\n",
    "    \n",
    "    best_config = [data_type[i](c) for i, c in enumerate(best_config)]\n",
    "        \n",
    "    return configurations, predictions, best_config, best_pred, pred_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40965f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, dropout_prob=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.first_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.last_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.first_layer(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.last_layer(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def ML_XGBoost():\n",
    "    return xgboost.XGBRegressor()\n",
    "\n",
    "def _train_nn(model, feature, target, epochs = 100):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss() \n",
    "    regression_batch = 16\n",
    "    training_losses = []\n",
    "    Batch = []\n",
    "    for indexer in range((len(feature) // regression_batch) + 1):\n",
    "        Batch.append([feature[indexer * regression_batch : (indexer+1) * regression_batch,:],\n",
    "                    target[indexer * regression_batch : (indexer+1) * regression_batch,:]])\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for fea, tar in Batch:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(feature)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(Batch)\n",
    "        if epoch % 10 == 0: print(f\"Epoch [{epoch}/{epochs}], Loss: {train_loss:.4f}\")    \n",
    "        training_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    return training_losses\n",
    "def _train_ml(model, feature, target):\n",
    "    model.fit(feature, target)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60036131",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./uploads/24-learning.csv\").drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f07adb42",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './24-learning.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./24-learning.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n\u001b[0;32m      4\u001b[0m unit \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m1000\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\" 원래 제약조건 \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mlower_bound = [0, 20, 0, 0, 20, 50, 10, 0, 0, 300, 0, 1, 20, 300, 0]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mupper_bound = [300, 40, 100, 5, 150, 1000, 60, 50, 80, 6000, 1, 5, 130, 6000, 20000]\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\flask\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\flask\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\flask\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\flask\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\flask\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './24-learning.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./24-learning.csv\").drop_duplicates()\n",
    "\n",
    "\n",
    "unit = [5, 5, 10, 0.5, 5, 5, 5, 5, 5, 60, 0.05, 1, 5, 60, 1000]\n",
    "\"\"\" 원래 제약조건 \n",
    "lower_bound = [0, 20, 0, 0, 20, 50, 10, 0, 0, 300, 0, 1, 20, 300, 0]\n",
    "upper_bound = [300, 40, 100, 5, 150, 1000, 60, 50, 80, 6000, 1, 5, 130, 6000, 20000]\n",
    "\"\"\"\n",
    "lower_bound = data.min(axis=0).tolist()[1:]  # 각 행의 최소값 리스트\n",
    "upper_bound = data.max(axis=0).tolist()[1:]  # 각 행의 최대값 리스트\n",
    "\n",
    "\n",
    "data_type = [int, int, int, float, int, int, int, int, int, int, float, int, int, int, int]\n",
    "decimal_place = [0, 0, -1, 1, 0, 0, 0, 0, 0, -1, 2, 0, 0, -1, 0]\n",
    "\n",
    "starting_point = [150, 25, 40, 1, 120, 250, 10, 25, 25, 900, 0.25, 2, 100, 1800, 2000]\n",
    "\n",
    "\n",
    "\"\"\" test \"\"\"\n",
    "data_target = data['Target']\n",
    "data_feature = data.drop(columns=['Target'])\n",
    "erase = []\n",
    "u_, l_, b_, d_, p_, s_ = [], [], [], [], [], []\n",
    "\n",
    "for i in range(data_feature.shape[1]):\n",
    "    if min(data.iloc[:, i]) == max(data.iloc[:, i]):\n",
    "        erase.append(f\"att{i+1}\")\n",
    "    else:\n",
    "        u_.append(unit[i])\n",
    "        l_.append(lower_bound[i])\n",
    "        b_.append(upper_bound[i])\n",
    "        d_.append(data_type[i])\n",
    "        p_.append(decimal_place[i])\n",
    "        s_.append(starting_point[i])\n",
    "\n",
    "unit = u_\n",
    "lower_bound = l_\n",
    "upper_bound = b_\n",
    "data_type = d_\n",
    "decimal_place = p_\n",
    "starting_point = s_\n",
    "\n",
    "data_feature = data_feature.drop(columns=erase)\n",
    "data = pd.concat([data_target, data_feature], axis=1)\n",
    "\n",
    "target = MinMaxScaling(data_target)\n",
    "feature = MinMaxScaling(data_feature)\n",
    "\n",
    "models = []\n",
    "model = MLP(input_size=data_feature.shape[1], output_size=1, hidden_size=64)\n",
    "_train_nn(model, feature.data, target.data)\n",
    "models.append(model)\n",
    "\n",
    "model = ML_XGBoost()\n",
    "_train_ml(model, feature.data, target.data)\n",
    "models.append(model)\n",
    "print(unit)\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(data_type)\n",
    "print(decimal_place)\n",
    "print(starting_point)\n",
    "print(erase)\n",
    "print(data_feature.shape)\n",
    "print(data_target.shape)\n",
    "print(data.shape)\n",
    "\"\"\" test\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21358fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 0.5, 5, 5, 60, 0.05, 1, 5, 1000]\n",
      "[10.0, 20.0, 0.1, 1.0, 35.0, 360.0, 0.5, 9.286, 100.0, 7200.0]\n",
      "[300.0, 20.0, 0.1, 1.0, 53.0, 1800.0, 1.0, 12.736, 100.0, 7200.0]\n",
      "[<class 'int'>, <class 'int'>, <class 'float'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "[0, 0, 1, 0, 0, -1, 2, 0, 0, 0]\n",
      "[150, 25, 1, 250, 25, 900, 0.25, 2, 100, 2000]\n",
      "[MLP(\n",
      "  (first_layer): Linear(in_features=10, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (last_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      "), XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=None, ...)]\n"
     ]
    }
   ],
   "source": [
    "print(unit)\n",
    "print(lower_bound)\n",
    "print(upper_bound)\n",
    "print(data_type)\n",
    "print(decimal_place)\n",
    "print(starting_point)\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a53df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ff81f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of feature  :  10\n",
      "Step 10 Target : 510, Prediction : 1466.0\n",
      "Step 20 Target : 510, Prediction : 1411.0\n",
      "Step 30 Target : 510, Prediction : 1435.0\n",
      "Step 40 Target : 510, Prediction : 1487.0\n",
      "Step 50 Target : 510, Prediction : 1491.0\n",
      "Step 60 Target : 510, Prediction : 1427.0\n",
      "Step 70 Target : 510, Prediction : 1419.0\n",
      "Step 80 Target : 510, Prediction : 1402.0\n",
      "Step 90 Target : 510, Prediction : 1486.0\n",
      "Step 100 Target : 510, Prediction : 1427.0\n",
      "Step 110 Target : 510, Prediction : 1474.0\n",
      "Step 120 Target : 510, Prediction : 1416.0\n",
      "Step 130 Target : 510, Prediction : 1362.0\n",
      "Step 140 Target : 510, Prediction : 1471.0\n",
      "Step 150 Target : 510, Prediction : 1490.0\n",
      "Step 160 Target : 510, Prediction : 1412.0\n",
      "Step 170 Target : 510, Prediction : 1365.0\n",
      "Step 180 Target : 510, Prediction : 1443.0\n",
      "Step 190 Target : 510, Prediction : 1440.0\n",
      "Step 200 Target : 510, Prediction : 1438.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "desired = 510\n",
    "mode = 'local'\n",
    "modeling = 'Single'\n",
    "strategy = 'stochastic'\n",
    "tolerance = 1\n",
    "beam_width = 5\n",
    "num_candidates = 5\n",
    "escape = True\n",
    "top_k = 2\n",
    "index = 0\n",
    "up = True\n",
    "alternative = 'keep_move'\n",
    "\n",
    "configurations, predictions, best_config, best_pred, pred_all  = run(data = data, models = models,\n",
    "                                                          desired = desired,\n",
    "                                                          starting_point = starting_point, \n",
    "                                                          mode = mode, modeling = modeling,\n",
    "                                                          strategy = strategy, tolerance = tolerance, \n",
    "                                                          beam_width = beam_width,\n",
    "                                                          num_cadidates = num_candidates, escape = escape, \n",
    "                                                          top_k = top_k, index = index,\n",
    "                                                          up = up, alternative = alternative,\n",
    "                                                          unit = unit,\n",
    "                                                          lower_bound = lower_bound, \n",
    "                                                          upper_bound = upper_bound, \n",
    "                                                          data_type = data_type, decimal_place = decimal_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "791b670b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[150, 20, 0.1, 1, 21, 20, 900, 0.2, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 25, 900, 0.2, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 25, 900, 0.15, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.1, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.1, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 0, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 0, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 0, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 0, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 25, 900, 0.2, 0, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 0, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.2, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.15, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 900, 0.1, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 840, 0.1, 1, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 780, 0.1, 3, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 780, 0.15, 2, 100, 2000],\n",
       " [145, 20, 0.1, 1, 21, 20, 780, 0.15, 3, 100, 2000],\n",
       " [140, 20, 0.1, 1, 21, 20, 780, 0.15, 2, 100, 2000],\n",
       " [135, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [135, 20, 0.1, 1, 21, 20, 780, 0.15, 2, 100, 2000],\n",
       " [135, 20, 0.1, 1, 21, 20, 780, 0.15, 2, 100, 2000],\n",
       " [130, 20, 0.1, 1, 21, 20, 780, 0.15, 2, 100, 2000],\n",
       " [130, 20, 0.1, 1, 21, 20, 780, 0.15, 2, 100, 2000],\n",
       " [130, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [130, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [130, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [125, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [125, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 25, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 3, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 25, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 25, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [125, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 25, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 25, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 3, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 25, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.1, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.15, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.15, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.15, 3, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 25, 780, 0.15, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.15, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.15, 3, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.15, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.15, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 2, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 1, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 1, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 1, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 1, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 0, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 0, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 25, 780, 0.2, 0, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 0, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 0, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 0, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 1, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.2, 1, 100, 2000],\n",
       " [120, 20, 0.1, 1, 21, 20, 780, 0.25, 1, 100, 2000]]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "93d93f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "167ae8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "4a0d032b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[826.0,\n",
       " 665.0,\n",
       " 453.0,\n",
       " 432.0,\n",
       " 505.0,\n",
       " 569.0,\n",
       " 538.0,\n",
       " 595.0,\n",
       " 551.0,\n",
       " 433.0,\n",
       " 547.0,\n",
       " 495.0,\n",
       " 540.0,\n",
       " 474.0,\n",
       " 764.0,\n",
       " 714.0,\n",
       " 477.0,\n",
       " 588.0,\n",
       " 652.0,\n",
       " 400.0,\n",
       " 629.0,\n",
       " 487.0,\n",
       " 499.0,\n",
       " 715.0,\n",
       " 518.0,\n",
       " 588.0,\n",
       " 701.0,\n",
       " 388.0,\n",
       " 519.0,\n",
       " 857.0,\n",
       " 477.0,\n",
       " 696.0,\n",
       " 466.0,\n",
       " 699.0,\n",
       " 436.0,\n",
       " 559.0,\n",
       " 805.0,\n",
       " 422.0,\n",
       " 453.0,\n",
       " 450.0,\n",
       " 492.0,\n",
       " 658.0,\n",
       " 532.0,\n",
       " 497.0,\n",
       " 491.0,\n",
       " 565.0,\n",
       " 486.0,\n",
       " 714.0,\n",
       " 636.0,\n",
       " 518.0,\n",
       " 581.0,\n",
       " 763.0,\n",
       " 606.0,\n",
       " 389.0,\n",
       " 673.0,\n",
       " 562.0,\n",
       " 389.0,\n",
       " 466.0,\n",
       " 418.0,\n",
       " 440.0,\n",
       " 620.0,\n",
       " 667.0,\n",
       " 606.0,\n",
       " 793.0,\n",
       " 541.0,\n",
       " 381.0,\n",
       " 662.0,\n",
       " 640.0,\n",
       " 389.0,\n",
       " 528.0,\n",
       " 473.0,\n",
       " 389.0,\n",
       " 389.0,\n",
       " 638.0,\n",
       " 528.0,\n",
       " 381.0,\n",
       " 473.0,\n",
       " 757.0,\n",
       " 598.0,\n",
       " 473.0,\n",
       " 552.0,\n",
       " 474.0,\n",
       " 442.0,\n",
       " 524.0,\n",
       " 429.0,\n",
       " 456.0,\n",
       " 489.0,\n",
       " 459.0,\n",
       " 905.0,\n",
       " 713.0,\n",
       " 468.0,\n",
       " 642.0,\n",
       " 516.0,\n",
       " 463.0,\n",
       " 498.0,\n",
       " 605.0,\n",
       " 447.0,\n",
       " 479.0,\n",
       " 565.0,\n",
       " 683.0,\n",
       " 504.0,\n",
       " 479.0,\n",
       " 498.0,\n",
       " 465.0,\n",
       " 509.0]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "bc4a29e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[120, 20, 0.1, 1, 21, 20, 780, 0.25, 1, 100, 2000]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "5b545d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509.0"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "3d0797f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "e432c6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[657.0, 645.0],\n",
       " [657.0, 645.0],\n",
       " [658.0, 645.0],\n",
       " [658.0, 645.0],\n",
       " [674.0, 646.0],\n",
       " [673.0, 646.0],\n",
       " [675.0, 646.0],\n",
       " [675.0, 646.0],\n",
       " [660.0, 645.0],\n",
       " [660.0, 645.0],\n",
       " [661.0, 645.0],\n",
       " [661.0, 645.0],\n",
       " [675.0, 658.0],\n",
       " [674.0, 658.0],\n",
       " [677.0, 658.0],\n",
       " [676.0, 658.0]]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b783f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8f7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
